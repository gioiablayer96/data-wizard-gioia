{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Prediction task**\n",
    "\n",
    "We are interested in predicting the future income from a user.\n",
    "1. Please create a prediction model, aiming to predict the target variable (org_price_usd_following_30_days). Use the train set for training a model, aiming to minimize RMSE of predictions over the test set.\n",
    "2. What are the three most important features that contributed to the prediction?\n",
    "\n",
    "Note: the following columns are related to the next task, and should not be used in the current task: ”treatment”, “org price usd following 30 days after impact”."
   ],
   "id": "75f9e10aa55bc15c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# run the following chunk to install dependencies (pay attention to lightgbm installation)\n",
    "# ! pip install pandas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# or run this in your terminal: pip install -r /path/to/requirements.txt"
   ],
   "id": "a1bf236eeec2dd97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "libomp installation for lightgbm:\n",
    "\n",
    "1. run *brew install libomp* in your terminal\n",
    "2. copy variables into your shell configuration (you should get them at the end of the libomp brew installation):\n",
    "\n",
    "    echo 'export LDFLAGS=\"-L/usr/local/opt/libomp/lib\"' >> ~/.zshrc\n",
    "\n",
    "    echo 'export CPPFLAGS=\"-I/usr/local/opt/libomp/include\"' >> ~/.zshrc\n",
    "\n",
    "    source ~/.zshrc\n",
    "\n",
    "3. run *pip install lightgbm --no-binary lightgbm* in your terminal"
   ],
   "id": "b8fddc3f8d47e276"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:17:28.945980Z",
     "start_time": "2024-12-25T15:17:25.647979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor ### need brew install libomp, then copy variables to shell configuration then run pip install lightgbm --no-binary lightgbm\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "print(\"LightGBM successfully imported!\")\n"
   ],
   "id": "6cc2f44c8e172ec9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM successfully imported!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:17:33.737085Z",
     "start_time": "2024-12-25T15:17:31.346116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv('train_home_assignment_.csv', index_col=0)\n",
    "df_test = pd.read_csv('test_home_assignment.csv')"
   ],
   "id": "8dadd1ff758fc32e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:17:36.794809Z",
     "start_time": "2024-12-25T15:17:36.720966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop columns related to next task\n",
    "df_train.drop(columns=['treatment', 'org_price_usd_following_30_days_after_impact'], inplace=True)"
   ],
   "id": "87c19e71e55aa844",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I'll be carrying EDA, Feature treatment (preprocessing, engineering and selection based on feature importance) on the train set. I assume train and test set have the same prediction point, meaning the explanatory features available in the train set will match the prediction point in the user funnel within the gaming app where my model will have to return the prediction for new data (test set). For example, if the user enters the gaming app, plays and we want to return a prediction before the next level unlocks, I assume that the features available in this dataset are all available before the next level unlocks. In any case the dimensions of train and test the datasets match (with the same columns), so I assume the features used in the train set match the prediction point of the test set.\n",
    "\n",
    "I'll further split the train set into train and validation, to assess first of all the performance of the model on the validation set and use the latter for tuning eventual hyperparameters. I'll leave the test set untouched, and I'll address it as new data, meaning the test set will go through relevant transformations or preprocessing based on the train set before inputting into the tuned model for final predictions.\n",
    "\n",
    "Finally, I'll evaluate the tuned model with the chosen features on the test set by comparing the RMSE and other relevant metrics between different learners."
   ],
   "id": "30a0ebffe24080a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:17:41.165728Z",
     "start_time": "2024-12-25T15:17:41.160868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#check if we have the same columns in train and test\n",
    "# Get the column sets\n",
    "train_columns = set(df_train.columns)\n",
    "test_columns = set(df_test.columns)\n",
    "\n",
    "# Check for equality\n",
    "if train_columns == test_columns:\n",
    "    print(\"The columns in df_train and df_test are the same.\")\n",
    "else:\n",
    "    print(\"The columns in df_train and df_test are different.\")\n",
    "\n",
    "    # Find columns in df_train but not in df_test\n",
    "    missing_in_test = train_columns - test_columns\n",
    "    if missing_in_test:\n",
    "        print(f\"Columns in df_train but not in df_test: {missing_in_test}\")\n",
    "\n",
    "    # Find columns in df_test but not in df_train\n",
    "    missing_in_train = test_columns - train_columns\n",
    "    if missing_in_train:\n",
    "        print(f\"Columns in df_test but not in df_train: {missing_in_train}\")"
   ],
   "id": "52c404544ee0684f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns in df_train and df_test are the same.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "EDA\n",
    "\n",
    "In this section I use sweetviz, a module that returns a html with a comprehensive exploratory data analysis including:\n",
    "- marginal and joint distributions of Y, continuous features and categorical features\n",
    "- measures of associations\n",
    "- the goal here is to:\n",
    "    - check eventual missing values (for imputation)\n",
    "    - check statistical outliers for capping or removal\n",
    "    - look at the correlations:\n",
    "        - with Y, to have a first glimpse of the predictive power of the feature (we ideally want features in X being highly correlated with Y in its absolute value).\n",
    "        - between features, to get an idea if we need to introduce interaction variables for highly correlated explanatory variables (ideally we want explanatory features independent between each other, so we do not want multicollinearity)\n",
    "    - check if we have constant features, which having 0 variance are not explanatory at all\n",
    "    - the following checks will help us chose whether we are in a linear or non-linear setting, and therefore the choice of the learners\n",
    "        - check the skewness of the label to assess if a transformation is needed\n",
    "        - check the scale of the features and the label.\n",
    "        - check the relationship between Y and the features\n",
    "    - check cardinality of categorical features for binning or one-hot-encoding\n",
    "    - check if we have duplicate rows"
   ],
   "id": "ce838e88740e396"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:19:01.526752Z",
     "start_time": "2024-12-25T15:17:48.055797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify data types. everything that is count, total, occurrence, price, number of days - I address it as continuous\n",
    "for col in df_train.columns:\n",
    "    if col in ['weekday', 'village_id']:\n",
    "        df_train[col] = df_train[col].astype('category')\n",
    "    else:\n",
    "        df_train[col] = df_train[col].astype(float)\n",
    "\n",
    "\n",
    "report = sv.analyze([df_train, \"Train\"], target_feat=\"org_price_usd_following_30_days\")\n",
    "report.show_html(\"regression_report.html\")"
   ],
   "id": "badf0ce77f3894a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done! Use 'show' commands to display/save.   |██████████| [100%]   00:06 -> (00:00 left)                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report regression_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:19:12.589766Z",
     "start_time": "2024-12-25T15:19:11.226155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# further check correlations\n",
    "correlation_matrix = df_train.corr()\n",
    "print(correlation_matrix['org_price_usd_following_30_days'].sort_values(ascending=False))"
   ],
   "id": "ac48f4af9f721ca9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org_price_usd_following_30_days                                1.000000\n",
      "org_price_usd_preceding_30_days                                0.728005\n",
      "spins_reward_preceding_30_days                                 0.716380\n",
      "pet_xp_reward_preceding_30_days                                0.689277\n",
      "org_price_usd_preceding_7_to_30_days                           0.675280\n",
      "org_price_usd_triple_preceding_30_days                         0.633510\n",
      "payment_occurrences_preceding_30_days                          0.619148\n",
      "tournament_spins_reward_7_preceding                            0.612171\n",
      "org_price_usd_preceding_3_days                                 0.583488\n",
      "org_price_usd_preceding_3_to_7_days                            0.577685\n",
      "payment_occurrences_preceding_7_to_30_days                     0.568057\n",
      "chests_reward_preceding_30_days                                0.508032\n",
      "payment_occurrences_preceding_3_days                           0.458481\n",
      "payment_occurrences_preceding_3_to_7_days                      0.452433\n",
      "tournament_chest_reward_7_preceding                            0.274798\n",
      "total_pets_feed_preceding_7_days                               0.266364\n",
      "avg_past_seen_price_oos_preceding_7_days                       0.255087\n",
      "tournament_number_of_rank_in_top_10_7_preceding                0.224986\n",
      "ltv_gross_up_to_preceding_30_days                              0.220045\n",
      "total_spins_preceding_7_days                                   0.171660\n",
      "total_raids_preceding_7_days                                   0.168987\n",
      "total_attacks_preceding_7_days                                 0.163449\n",
      "activity_occurrences_preceding_7_days                          0.158202\n",
      "village_id                                                     0.151502\n",
      "total_villages_completed_preceding_7_days                      0.091171\n",
      "total_oos_action_fire_preceding_7_days                         0.088311\n",
      "total_set_completed_preceding_7_days                           0.086450\n",
      "total_cards_collected_preceding_7_days                         0.085794\n",
      "total_card_xp_diff_preceding_7_days                            0.085665\n",
      "active_days_preceding_7_days                                   0.079151\n",
      "total_card_xp                                                  0.078008\n",
      "total_raids_on_user_preceding_7_days                           0.069971\n",
      "tournament_coins_reward_7_preceding                            0.064524\n",
      "raids_amount_normalized_preceding_7_days                       0.062804\n",
      "total_new_cards_collected_chest_from_store_preceding_7_days    0.053312\n",
      "active_days_preceding_7_to_14_days                             0.045272\n",
      "avg_spin_aggressiveness_oos_preceding_7_days                   0.038265\n",
      "total_oos_action_fire                                          0.030091\n",
      "total_attacks_on_user_preceding_7_days                         0.029826\n",
      "total_friend_link_invites_preceding_7_days                     0.029099\n",
      "chests_amount_normalized_preceding_7_days                      0.022657\n",
      "total_villages_completed                                       0.021229\n",
      "hours_since_installed_ma                                       0.020791\n",
      "spins_inventory_preceding_30_days                              0.017217\n",
      "total_payers_friends_active_in_the_last_7days                  0.016817\n",
      "total_cards_collected_inbox_preceding_7_days                   0.016591\n",
      "total_set_completed                                            0.009643\n",
      "total_friend_link_invites                                      0.007005\n",
      "total_friends_active_in_the_last_7days                        -0.000907\n",
      "weekday                                                       -0.005283\n",
      "total_repairs_preceding_7_days                                -0.008685\n",
      "total_friends_active_in_the_last_7_to_14_days                 -0.009144\n",
      "hours_in_village                                              -0.027576\n",
      "spins_rewards_lo_preceding_7_days                                   NaN\n",
      "Name: org_price_usd_following_30_days, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Observations from EDA (in case the html does not work, I added the pictures_EDA folder with relevant screenshots):\n",
    "\n",
    "1. The Label, in black: first of all, it is very skewed (43.3% are zeros). And this is reflected by the skewness of 22.5. With a skewness this high, a transformation like log or sqrt will decrease the skewness but won't make the distribution symmetrical, which means that maybe a linear setting might not be ideal.\n",
    "2. For each feature, the report gives the following indexes to measure correlations:\n",
    "\n",
    "    a. Theil's U uncertainty coefficient: between categorical variables\n",
    "\n",
    "        - values are very low - which means that the features are independent, but they are weekly correlated with Y.\n",
    "\n",
    "    b. Correlation Ratio (chi squared): between categorical and numerical variables\n",
    "\n",
    "        - the 6 categorical variables show a very weak association with Y. I am going to drop them when predicting.\n",
    "\n",
    "    c. Pearson Correlation: between numerical variables\n",
    "\n",
    "        - we can see that there are 2 features which are highly correlated with the label: org_price_usd_preceding_30_days (0.73), spins_reward_preceding_30_days (0.72).\n",
    "        - below them, 9 more features with moderate correlation (between 0.5 and 0.7)\n",
    "        - we do have multicolinearity, for example we have org_price_usd_preceding_30_days being highly correlated to org_price_usd_preceding_7_to_30_days (0.98), spins_reward_preceding_30_days (0.97), pet_xp_reward_preceding_30_days (0.92), org_price_usd_triple_preceding_30_days (0.87), payment_occurrences_preceding_30_days (0.84), payment_occurrences_preceding_7_to_30_days (0.82)\n",
    "\n",
    "3. A short explanation about the plots: each feature is depicted with both a histogram (feature values on the x axis, frequency (%) of feature value on the left vertical axis) and a line plot with aggregated data (feature values on the x axis, average Y value on the right vertical axis). We can see that many of the features present a non-linear relationship with Y.\n",
    "\n",
    "\n",
    "ACTION ITEMS\n",
    "- drop duplicates rows\n",
    "- drop constant feature: spins_rewards_lo_preceding_7_days\n",
    "- 'weekday' shows a low association with Y, I'll drop it before prediction. There are some features that sweetviz addressed as categorical: ['active_days_preceding_7_days', 'active_days_preceding_7_to_14_days', 'total_set_completed', 'total_friend_link_invites']. I am not sure they are, so I computed for them the pearson correlation which still resulted very low, so I'll be dropping them as well. Therefore I'll be dropping the following variables: ['active_days_preceding_7_days', 'active_days_preceding_7_to_14_days', 'weekday', 'total_set_completed', 'total_friend_link_invites'].\n",
    "- village_id shows a moderate correlation with Y, I'll bin it according to sweetviz suggestion into less categories considering the high cardinality and I apply OHE, leaving out the \"other\" category for redundancy.\n",
    "- we don't have missing values. so no imputation needed.\n",
    "\n",
    "Considering the setting is non-linear, that features and label have different scales, that we might have outliers given the presence of high average Y values in the line plots, and considering the presence of highly correlated features, it seems like tree-based learners would be a better fit for this problem, as they are more suitable for non-linear relations between features and Y (as shown in the plots), they do not need scaling, they are robust to outliers and to highly correlated features, and they better handle the skewness."
   ],
   "id": "ecc154e0b3195e46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:19:31.917538Z",
     "start_time": "2024-12-25T15:19:31.319015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# preprocessing\n",
    "df_train.drop(columns=['spins_rewards_lo_preceding_7_days', 'active_days_preceding_7_days', 'active_days_preceding_7_to_14_days', 'weekday', 'total_set_completed', 'total_friend_link_invites'], inplace=True)\n",
    "df_train.drop_duplicates(inplace=True)"
   ],
   "id": "279f7ad241e8691d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:19:35.096989Z",
     "start_time": "2024-12-25T15:19:34.436377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train['village_id'] = df_train['village_id'].astype(int).astype(str)\n",
    "# Step 1: Extract the village IDs that appear on the axis from the picture\n",
    "village_ids_to_keep = ['170', '165', '168', '169', '166', '163', '164', '171', '172', '161', '173', '162', '176']\n",
    "\n",
    "# Step 2: Bin the village_id column\n",
    "df_train['village_id_binned'] = df_train['village_id'].apply(\n",
    "    lambda x: x if x in village_ids_to_keep else 'Other'\n",
    ")\n",
    "df_train = pd.get_dummies(df_train, columns=['village_id_binned']).drop(columns=['village_id_binned_Other','village_id'])"
   ],
   "id": "c405621af0674dcf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Further split train into train-dev",
   "id": "dba8645824637067"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:19:38.700641Z",
     "start_time": "2024-12-25T15:19:38.543558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#split into train-dev set\n",
    "X = df_train.drop(columns=['org_price_usd_following_30_days'])\n",
    "y = df_train['org_price_usd_following_30_days']\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "a3ada7cc1246d1a6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tree based feature importance",
   "id": "c7fe3d49d49d7ba6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:21:11.127991Z",
     "start_time": "2024-12-25T15:19:46.381313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "importances = gb.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "cumulative_importance = np.cumsum(importances[sorted_indices])\n",
    "threshold_index = np.where(cumulative_importance >= 0.95)[0][0]\n",
    "selected_features = X.columns[sorted_indices[:threshold_index + 1]]"
   ],
   "id": "173b76d93fb49ef2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:21:11.198498Z",
     "start_time": "2024-12-25T15:21:11.194479Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Selected Features (95% cumulative importance): {list(selected_features)}\")",
   "id": "8a869df72f7618d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (95% cumulative importance): ['org_price_usd_preceding_30_days', 'org_price_usd_preceding_3_days', 'spins_reward_preceding_30_days', 'tournament_spins_reward_7_preceding', 'org_price_usd_preceding_7_to_30_days', 'pet_xp_reward_preceding_30_days', 'org_price_usd_triple_preceding_30_days', 'org_price_usd_preceding_3_to_7_days', 'tournament_coins_reward_7_preceding', 'payment_occurrences_preceding_3_days', 'hours_since_installed_ma', 'ltv_gross_up_to_preceding_30_days', 'payment_occurrences_preceding_30_days', 'chests_reward_preceding_30_days', 'total_attacks_preceding_7_days', 'total_villages_completed_preceding_7_days']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:22:15.841131Z",
     "start_time": "2024-12-25T15:22:15.808250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_selected = X_train[selected_features]\n",
    "X_dev_selected = X_dev[selected_features]"
   ],
   "id": "5222f5a174550f12",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Reasons why I used Gradient Boosting for feature selection:\n",
    "\n",
    "Random Forest is less suitable for feature selection because it builds trees independently, which means that if we have multiple correlated features, they can be selected in different trees and inflate the importance. GB instead works sequentially, since each tree improves the performance of the previous one based on high residuals. this means that if we have 2 features that are highly correlated, GB selects one to avoid redundancy. The feature selected is the one that reduces the residual error at each step. Then once one feature in a correlated group is selected, the others offer diminishing returns in terms of residual reduction."
   ],
   "id": "41784d731d1258f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:22:17.662626Z",
     "start_time": "2024-12-25T15:22:17.656653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare assessment metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    return {\"RMSE\": rmse}"
   ],
   "id": "ad610bbbf79a8eca",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:25:42.282305Z",
     "start_time": "2024-12-25T15:22:22.378013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Algorithms to evaluate (adaboost does not handle well multicolinearity)\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(random_state=42),\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
    "}\n",
    "\n",
    "# Evaluate each model on dev set\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_dev_selected)\n",
    "    metrics = evaluate_metrics(y_dev, y_pred)\n",
    "    results[name] = metrics\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "    print()"
   ],
   "id": "118e139d9417a8dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3454\n",
      "[LightGBM] [Info] Number of data points in the train set: 159996, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 43.173697\n",
      "Model: RandomForest\n",
      "  RMSE: 109.6293\n",
      "\n",
      "Model: GradientBoosting\n",
      "  RMSE: 108.9748\n",
      "\n",
      "Model: XGBoost\n",
      "  RMSE: 109.0955\n",
      "\n",
      "Model: LightGBM\n",
      "  RMSE: 107.8366\n",
      "\n",
      "Model: CatBoost\n",
      "  RMSE: 106.0509\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:29:21.087222Z",
     "start_time": "2024-12-25T15:25:42.426631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate each model on train set to check overfitting\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_train_selected)\n",
    "    metrics = evaluate_metrics(y_train, y_pred)\n",
    "    results[name] = metrics\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "    print()"
   ],
   "id": "fbe65315f3d70e2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3454\n",
      "[LightGBM] [Info] Number of data points in the train set: 159996, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 43.173697\n",
      "Model: RandomForest\n",
      "  RMSE: 40.2751\n",
      "\n",
      "Model: GradientBoosting\n",
      "  RMSE: 89.1578\n",
      "\n",
      "Model: XGBoost\n",
      "  RMSE: 64.5069\n",
      "\n",
      "Model: LightGBM\n",
      "  RMSE: 86.1012\n",
      "\n",
      "Model: CatBoost\n",
      "  RMSE: 69.3761\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So far it seems like GB is the less prone to overfitting, because of the lowest discrepancy between train and dev RMSE.",
   "id": "15a04c8d4762c603"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hyperparameter tuning on dev set through RandomSearchCV (for speed, as GridSearchCV takes time) - done on GB.",
   "id": "d0f8e602456ecf4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:29:40.021314Z",
     "start_time": "2024-12-25T15:29:39.946683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gb_params = {\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"subsample\": [0.7, 0.8, 1.0]\n",
    "}"
   ],
   "id": "6a335cd1f1334e55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlearning_rate: Smaller values reduce overfitting but require more trees.\\nn_estimators: Number of boosting rounds.\\nmax_depth: Maximum depth of trees to control overfitting.\\nsubsample: Fraction of samples used for training each tree. Lower values reduce overfitting.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "- learning_rate: Smaller values reduce overfitting but require more trees.\n",
    "- n_estimators: Number of boosting rounds.\n",
    "- max_depth: Maximum depth of trees to control overfitting.\n",
    "- subsample: Fraction of samples used for training each tree. Lower values reduce overfitting."
   ],
   "id": "728fa9a89a908578"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:36:31.989054Z",
     "start_time": "2024-12-25T15:30:18.077901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "gb_random_search = RandomizedSearchCV(\n",
    "    estimator=gb,\n",
    "    param_distributions=gb_params,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_random_search.fit(X_dev_selected, y_dev)\n",
    "print(\"Best Parameters for Gradient Boosting:\", gb_random_search.best_params_)\n",
    "print(\"Best CV RMSE:\", -gb_random_search.best_score_)"
   ],
   "id": "13cb2561d1b4f03d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Parameters for Gradient Boosting: {'subsample': 0.7, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "Best CV RMSE: 108.87252309616323\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Predict",
   "id": "c6350ea9f9e4a33f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:37:37.220407Z",
     "start_time": "2024-12-25T15:37:36.859174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_test = df_test.drop(columns=['org_price_usd_following_30_days'])\n",
    "y_test = df_test['org_price_usd_following_30_days']\n",
    "\n",
    "# apply feature selection (in the end the indicators village_id did not come up in the FI so I just select the important features without further need of binning or encoding)\n",
    "X_test_selected = X_test[selected_features]"
   ],
   "id": "742122b076d4e798",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:38:04.390005Z",
     "start_time": "2024-12-25T15:37:43.842167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GradientBoostingRegressor(subsample=0.7, n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train_selected, y_train)\n",
    "y_pred = model.predict(X_test_selected)\n",
    "metrics = evaluate_metrics(y_test, y_pred)\n",
    "print(metrics)"
   ],
   "id": "fbef3bafcf356719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RMSE': 108.80508623337772}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- What are the three most important features that contributed to the prediction?\n",
    "\n",
    "The top 3 features that contributed to the prediction are: 'payment_occurrences_preceding_30_days', 'total_raids_preceding_7_days' and 'hours_since_installed_ma'."
   ],
   "id": "cfe4592da018904b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T15:38:20.464264Z",
     "start_time": "2024-12-25T15:38:20.434827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# top 3 most important features\n",
    "importances = model.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "selected_features = X.columns[sorted_indices[:3]]\n",
    "selected_features"
   ],
   "id": "b5a0f891d5996047",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['payment_occurrences_preceding_30_days', 'total_raids_preceding_7_days',\n",
       "       'hours_since_installed_ma'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TODO\n",
    "\n",
    "REFACTORING: save the best model in a pickle file in a folder that is automatically created in the current working directory. reorganize code into class. use pipeline when you can (& select from model for feat selection). prepare requirements/pip install chunk and explanation on how to install lightGBM with libomp. within the class you need to have also a preprocessing for the test, matching the train, and output the top 3 features. create a PDF in case installations on their side do not work."
   ],
   "id": "888f4939d01cff54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "60e44e2198b3052e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Recommendation task**\n",
    "\n",
    "We are interested in increasing the income from users. For that, we ran a randomized experiment where the population was given either a 10 usd offer or 2usd offer (see the \"treatment\" column), aiming to learn what offer should be given to a user. The experiment yielded a target variable named “org_price_usd_following_30_days_after_impact”, reflecting the result of the experiment in terms of income.\n",
    "1. For each user in the test data, set the treatment (either 10 or 2) that you believe would maximize the target variable (add a new column for that)\n",
    "2. What are the three most important features that contributed to the decision to give users a specific treatment?"
   ],
   "id": "a2d6dd1c556b191a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:03:36.710031Z",
     "start_time": "2024-12-25T16:03:34.176616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv('train_home_assignment_.csv', index_col=0)\n",
    "df_test = pd.read_csv('test_home_assignment.csv')"
   ],
   "id": "6cc563f427c7d94b",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:03:37.949884Z",
     "start_time": "2024-12-25T16:03:37.901340Z"
    }
   },
   "cell_type": "code",
   "source": "df_train[['org_price_usd_following_30_days','org_price_usd_following_30_days_after_impact']].corr()",
   "id": "2923fed586802501",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              org_price_usd_following_30_days  \\\n",
       "org_price_usd_following_30_days                                      1.000000   \n",
       "org_price_usd_following_30_days_after_impact                         0.999656   \n",
       "\n",
       "                                              org_price_usd_following_30_days_after_impact  \n",
       "org_price_usd_following_30_days                                                   0.999656  \n",
       "org_price_usd_following_30_days_after_impact                                      1.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_price_usd_following_30_days</th>\n",
       "      <th>org_price_usd_following_30_days_after_impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>org_price_usd_following_30_days</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org_price_usd_following_30_days_after_impact</th>\n",
       "      <td>0.999656</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:03:39.851159Z",
     "start_time": "2024-12-25T16:03:39.700415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dropping old label\n",
    "df_train.drop(columns=['org_price_usd_following_30_days'], inplace=True)\n",
    "df_test.drop(columns=['org_price_usd_following_30_days'], inplace=True)"
   ],
   "id": "50136f657c7ac9ba",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From this task I understand that I need to recommend a treatment to each new user of the test set under the constraint of maximizing org_price_usd_following_30_days_after_impact.",
   "id": "1ad2c3a2d26a20b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:03:43.734019Z",
     "start_time": "2024-12-25T16:03:43.708279Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.groupby(['treatment'])['org_price_usd_following_30_days_after_impact'].sum()",
   "id": "d6cba5d0fa7ec81f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "treatment\n",
       "2     4.313935e+06\n",
       "10    4.399805e+06\n",
       "Name: org_price_usd_following_30_days_after_impact, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "By looking at the sum of org_price_usd_following_30_days_after_impact for each treatment, it seems like treatment does not have any disciminatory power, meaning including it as a further feature in a predictive model won't help much. Therefore, I'll approach this problem in the following way:\n",
    "- I'll train 2 models, one for each treatment, where the label is org_price_usd_following_30_days_after_impact.\n",
    "- I'll output 2 predictions for the test set, one per model\n",
    "- For each new user in the test set, I'll assign 10 if the prediction for 10 is higher, otherwise 2.\n",
    "- I'll compute the total predicted outcome based on this method and I'll compare it to a random baseline."
   ],
   "id": "24d703eacbe9e9d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I'll use the previous preprocessing and tuned learner as we have a correlation of 0.99 between previous and current label.",
   "id": "1708d957840d876"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:03:57.928680Z",
     "start_time": "2024-12-25T16:03:56.900377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "df_train.drop(\n",
    "    columns=['spins_rewards_lo_preceding_7_days', 'active_days_preceding_7_days', 'active_days_preceding_7_to_14_days',\n",
    "             'weekday', 'total_set_completed', 'total_friend_link_invites'], inplace=True)\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "df_train['village_id'] = df_train['village_id'].astype(int).astype(str)\n",
    "# Step 1: Extract the village IDs that appear on the axis from the picture\n",
    "village_ids_to_keep = ['170', '165', '168', '169', '166', '163', '164', '171', '172', '161', '173', '162', '176']\n",
    "\n",
    "# Step 2: Bin the village_id column\n",
    "df_train['village_id_binned'] = df_train['village_id'].apply(\n",
    "    lambda x: x if x in village_ids_to_keep else 'Other'\n",
    ")\n",
    "df_train = pd.get_dummies(df_train, columns=['village_id_binned']).drop(\n",
    "    columns=['village_id_binned_Other', 'village_id'])"
   ],
   "id": "67d82bbb78f8fe1f",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:06:01.998324Z",
     "start_time": "2024-12-25T16:06:01.058368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test.drop(\n",
    "    columns=['spins_rewards_lo_preceding_7_days', 'active_days_preceding_7_days', 'active_days_preceding_7_to_14_days',\n",
    "             'weekday', 'total_set_completed', 'total_friend_link_invites'], inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert village_id in both train and test to string\n",
    "df_test['village_id'] = df_test['village_id'].astype(int).astype(str)\n",
    "\n",
    "# Bin the village_id column in the test set\n",
    "df_test['village_id_binned'] = df_test['village_id'].apply(\n",
    "    lambda x: x if x in village_ids_to_keep else 'Other'\n",
    ")\n",
    "\n",
    "# One-hot encode the binned village_id column\n",
    "village_id_dummies_test = pd.get_dummies(df_test['village_id_binned'], prefix='village_id_binned')\n",
    "\n",
    "# Ensure test set columns align with train set columns\n",
    "# Add missing columns from the training step\n",
    "for col in [f'village_id_binned_{v}' for v in village_ids_to_keep]:\n",
    "    if col not in village_id_dummies_test.columns:\n",
    "        village_id_dummies_test[col] = 0\n",
    "\n",
    "# Drop extra columns not in the training set\n",
    "village_id_dummies_test = village_id_dummies_test[\n",
    "    [f'village_id_binned_{v}' for v in village_ids_to_keep]\n",
    "]\n",
    "\n",
    "# Add the one-hot encoded columns to the test set\n",
    "df_test = pd.concat([df_test, village_id_dummies_test], axis=1)\n",
    "\n",
    "# Drop the original village_id and binned column\n",
    "df_test = df_test.drop(columns=['village_id', 'village_id_binned'], errors='ignore')\n"
   ],
   "id": "6d7bcf708b010135",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   payment_occurrences_preceding_30_days  hours_since_installed_ma  \\\n",
      "0                                    1.0                   33544.0   \n",
      "1                                   35.0                   33542.0   \n",
      "2                                    4.0                   33495.0   \n",
      "3                                    1.0                   33349.0   \n",
      "4                                   11.0                   33241.0   \n",
      "\n",
      "   total_raids_preceding_7_days  org_price_usd_preceding_3_days  \\\n",
      "0                         100.0                            0.00   \n",
      "1                          26.0                           62.94   \n",
      "2                           4.0                            0.00   \n",
      "3                          19.0                            3.99   \n",
      "4                          32.0                            0.00   \n",
      "\n",
      "   hours_in_village  total_card_xp  total_villages_completed_preceding_7_days  \\\n",
      "0              59.0            0.0                                        2.0   \n",
      "1              44.0            0.0                                        2.0   \n",
      "2             190.0            0.0                                        0.0   \n",
      "3              60.0            0.0                                        4.0   \n",
      "4              98.0         1975.0                                        1.0   \n",
      "\n",
      "   total_friends_active_in_the_last_7_to_14_days  \\\n",
      "0                                           14.0   \n",
      "1                                            7.0   \n",
      "2                                           25.0   \n",
      "3                                            0.0   \n",
      "4                                           24.0   \n",
      "\n",
      "   chests_reward_preceding_30_days  total_villages_completed  ...  \\\n",
      "0                              0.0                       0.0  ...   \n",
      "1                             10.0                       0.0  ...   \n",
      "2                              1.0                       0.0  ...   \n",
      "3                              0.0                       0.0  ...   \n",
      "4                              3.0                       0.0  ...   \n",
      "\n",
      "   village_id_binned_169  village_id_binned_166  village_id_binned_163  \\\n",
      "0                  False                   True                  False   \n",
      "1                  False                  False                  False   \n",
      "2                  False                  False                  False   \n",
      "3                  False                  False                  False   \n",
      "4                  False                  False                  False   \n",
      "\n",
      "   village_id_binned_164  village_id_binned_171  village_id_binned_172  \\\n",
      "0                  False                  False                  False   \n",
      "1                  False                  False                  False   \n",
      "2                  False                  False                  False   \n",
      "3                  False                  False                  False   \n",
      "4                  False                  False                  False   \n",
      "\n",
      "   village_id_binned_161  village_id_binned_173  village_id_binned_162  \\\n",
      "0                  False                  False                  False   \n",
      "1                  False                  False                  False   \n",
      "2                  False                  False                  False   \n",
      "3                  False                  False                  False   \n",
      "4                  False                  False                  False   \n",
      "\n",
      "   village_id_binned_176  \n",
      "0                  False  \n",
      "1                  False  \n",
      "2                  False  \n",
      "3                  False  \n",
      "4                  False  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:23:19.495029Z",
     "start_time": "2024-12-25T16:22:04.049877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Split the data into 10 and 2 treatment.\n",
    "df_train_10 = df_train[df_train.treatment == 10].drop(columns=['treatment'])\n",
    "df_train_2 = df_train[df_train.treatment == 2].drop(columns=['treatment'])\n",
    "\n",
    "# Separate data by treatment\n",
    "X_treatment_10 = df_train_10.drop(columns=['org_price_usd_following_30_days_after_impact'])\n",
    "y_treatment_10 = df_train_10['org_price_usd_following_30_days_after_impact']\n",
    "\n",
    "X_treatment_2 = df_train_2.drop(columns=['org_price_usd_following_30_days_after_impact'])\n",
    "y_treatment_2 = df_train_2['org_price_usd_following_30_days_after_impact']\n",
    "\n",
    "# Train models for each treatment\n",
    "model_10 = GradientBoostingRegressor(subsample=0.7, n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "model_10.fit(X_treatment_10, y_treatment_10)\n",
    "\n",
    "model_2 = GradientBoostingRegressor(subsample=0.7, n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "model_2.fit(X_treatment_2, y_treatment_2)\n",
    "\n"
   ],
   "id": "6af4b9a370b162fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(random_state=42, subsample=0.7)"
      ],
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(random_state=42, subsample=0.7)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GradientBoostingRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\">?<span>Documentation for GradientBoostingRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GradientBoostingRegressor(random_state=42, subsample=0.7)</pre></div> </div></div></div></div>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:23:19.636988Z",
     "start_time": "2024-12-25T16:23:19.628048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#check if we have the same columns in train and test\n",
    "# Get the column sets\n",
    "train_columns = set(X_treatment_10.columns)\n",
    "test_columns = set(df_test.columns)\n",
    "\n",
    "# Check for equality\n",
    "if train_columns == test_columns:\n",
    "    print(\"The columns in df_train and df_test are the same.\")\n",
    "else:\n",
    "    print(\"The columns in df_train and df_test are different.\")\n",
    "\n",
    "    # Find columns in df_train but not in df_test\n",
    "    missing_in_test = train_columns - test_columns\n",
    "    if missing_in_test:\n",
    "        print(f\"Columns in df_train but not in df_test: {missing_in_test}\")\n",
    "\n",
    "    # Find columns in df_test but not in df_train\n",
    "    missing_in_train = test_columns - train_columns\n",
    "    if missing_in_train:\n",
    "        print(f\"Columns in df_test but not in df_train: {missing_in_train}\")"
   ],
   "id": "413cb181a5c70fdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns in df_train and df_test are different.\n",
      "Columns in df_test but not in df_train: {'predicted_outcome', 'recommended_treatment'}\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:23:20.715142Z",
     "start_time": "2024-12-25T16:23:19.663329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predict for the test set\n",
    "df_test=df_test[X_treatment_10.columns]\n",
    "y_pred_10 = model_10.predict(df_test)\n",
    "y_pred_2 = model_2.predict(df_test)\n",
    "\n",
    "# Assign treatment based on predicted outcomes\n",
    "df_test[\"recommended_treatment\"] = [10 if pred_10 > pred_2 else 2 for pred_10, pred_2 in zip(y_pred_10, y_pred_2)]"
   ],
   "id": "d2bc59d50ebecc37",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:23:20.908640Z",
     "start_time": "2024-12-25T16:23:20.814476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate total predicted outcome for recommended treatments\n",
    "df_test[\"predicted_outcome\"] = [\n",
    "    pred_10 if treatment == 10 else pred_2\n",
    "    for treatment, pred_10, pred_2 in zip(df_test[\"recommended_treatment\"], y_pred_10, y_pred_2)\n",
    "]\n",
    "total_predicted_outcome = df_test[\"predicted_outcome\"].sum()\n",
    "print(f\"Total Predicted Outcome: {total_predicted_outcome}\")\n",
    "\n",
    "# Calculate uplift (compared to random assignment baseline)\n",
    "baseline_outcome = max(y_pred_10.mean(), y_pred_2.mean()) * len(df_test)\n",
    "uplift = total_predicted_outcome - baseline_outcome\n",
    "print(f\"Uplift: {uplift}\")"
   ],
   "id": "6d61ae1d47f6764",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Predicted Outcome: 10235817.600143086\n",
      "Uplift: 655191.747320734\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:23:21.028155Z",
     "start_time": "2024-12-25T16:23:21.014460Z"
    }
   },
   "cell_type": "code",
   "source": "df_test.groupby([\"recommended_treatment\"])['predicted_outcome'].sum()",
   "id": "7060f4a5c669e954",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recommended_treatment\n",
       "2     4.750004e+06\n",
       "10    5.485813e+06\n",
       "Name: predicted_outcome, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T16:23:21.254517Z",
     "start_time": "2024-12-25T16:23:21.228953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature importance for \\$10 treatment model\n",
    "importances_10 = model_10.feature_importances_\n",
    "features_10 = pd.DataFrame({\n",
    "    \"feature\": X_treatment_10.columns,\n",
    "    \"importance\": importances_10\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "print(\"Top features for \\$10 treatment:\")\n",
    "print(features_10.head(3))\n",
    "\n",
    "# Feature importance for \\$2 treatment model\n",
    "importances_2 = model_2.feature_importances_\n",
    "features_2 = pd.DataFrame({\n",
    "    \"feature\": X_treatment_2.columns,\n",
    "    \"importance\": importances_2\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "print(\"Top features for \\$2 treatment:\")\n",
    "print(features_2.head(3))\n"
   ],
   "id": "4e9c049839ffe693",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features for \\$10 treatment:\n",
      "                                 feature  importance\n",
      "36       org_price_usd_preceding_30_days    0.373253\n",
      "31  org_price_usd_preceding_7_to_30_days    0.202460\n",
      "3         org_price_usd_preceding_3_days    0.084992\n",
      "Top features for \\$2 treatment:\n",
      "                            feature  importance\n",
      "36  org_price_usd_preceding_30_days    0.495140\n",
      "3    org_price_usd_preceding_3_days    0.078902\n",
      "12  pet_xp_reward_preceding_30_days    0.048531\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For users that got treatment 10$, the most important features were:\n",
    "- org_price_usd_preceding_30_days\n",
    "- org_price_usd_preceding_7_to_30_days\n",
    "- org_price_usd_preceding_3_days\n",
    "\n",
    "For users that got treatment 2$, the most important features were:\n",
    "- org_price_usd_preceding_30_days\n",
    "- pet_xp_reward_preceding_30_days\n",
    "- org_price_usd_preceding_3_days\n",
    "\n",
    "Since I used 2 different models, it makes sense that I would get some features that reflect treatment-specific patterns, but overall it looks like recent price (org_price_usd_preceding_X_days) plays a big role in determining org_price_usd_following_30_days_after_impact."
   ],
   "id": "c9c4f391cebdab98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1c20e24e1011db1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
