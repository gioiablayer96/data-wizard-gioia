{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Prediction task**\n",
    "\n",
    "We are interested in predicting the future income from a user.\n",
    "1. Please create a prediction model, aiming to predict the target variable (org_price_usd_following_30_days). Use the train set for training a model, aiming to minimize RMSE of predictions over the test set.\n",
    "2. What are the three most important features that contributed to the prediction?\n",
    "\n",
    "Note: the following columns are related to the next task, and should not be used in the current task: ”treatment”, “org price usd following 30 days after impact”."
   ],
   "id": "75f9e10aa55bc15c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:42:22.769963Z",
     "start_time": "2024-12-26T21:42:21.008677Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pandas sweetviz scikit-learn xgboost catboost matplotlib lightgbm numpy",
   "id": "6cc2f44c8e172ec9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: sweetviz in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (1.5.2)\r\n",
      "Requirement already satisfied: xgboost in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (1.6.2)\r\n",
      "Requirement already satisfied: catboost in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (1.2.7)\r\n",
      "Requirement already satisfied: matplotlib in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (3.9.4)\r\n",
      "Requirement already satisfied: lightgbm in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (4.5.0)\r\n",
      "Requirement already satisfied: numpy in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from pandas) (2024.2)\r\n",
      "Requirement already satisfied: tqdm>=4.43.0 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from sweetviz) (4.67.1)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from sweetviz) (1.13.1)\r\n",
      "Requirement already satisfied: jinja2>=2.11.1 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from sweetviz) (3.1.4)\r\n",
      "Requirement already satisfied: importlib-resources>=1.2.0 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from sweetviz) (6.4.5)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\r\n",
      "Requirement already satisfied: graphviz in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from catboost) (0.20.3)\r\n",
      "Requirement already satisfied: plotly in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from catboost) (5.24.1)\r\n",
      "Requirement already satisfied: six in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from catboost) (1.17.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from matplotlib) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from matplotlib) (11.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from matplotlib) (3.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from jinja2>=2.11.1->sweetviz) (3.0.2)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages (from plotly->catboost) (9.0.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For LightGBM, you may need to install libomp via Homebrew:\n",
    "1. Run `brew install libomp` in your terminal.\n",
    "2. Export the variables suggested by brew to your shell configuration (e.g., .zshrc or .bashrc):\n",
    "    echo 'export LDFLAGS=\"-L/usr/local/opt/libomp/lib\"' >> ~/.zshrc\n",
    "    echo 'export CPPFLAGS=\"-I/usr/local/opt/libomp/include\"' >> ~/.zshrc\n",
    "    source ~/.zshrc\n",
    " 3. Then, install LightGBM with: `pip install lightgbm --no-binary lightgbm`"
   ],
   "id": "de4e8e5420d6ebe6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:42:26.205278Z",
     "start_time": "2024-12-26T21:42:22.777669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(\"LightGBM successfully imported!\")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pandas')"
   ],
   "id": "6e9dea2a2d95f2ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelbenmergui/Documents/Gioia/PycharmProjects/data-wizard-gioia/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM successfully imported!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:42:29.822908Z",
     "start_time": "2024-12-26T21:42:26.257430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv('train_home_assignment_.csv', index_col=0)\n",
    "df_test = pd.read_csv('test_home_assignment.csv')"
   ],
   "id": "8dadd1ff758fc32e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:42:29.930159Z",
     "start_time": "2024-12-26T21:42:29.852261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop columns related to next task\n",
    "df_train.drop(columns=['treatment', 'org_price_usd_following_30_days_after_impact'], inplace=True)"
   ],
   "id": "87c19e71e55aa844",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I'll be carrying EDA, Feature treatment (preprocessing, engineering and selection based on feature importance) on the train set. I assume train and test set have the same prediction point, meaning the explanatory features available in the train set will match the prediction point in the user funnel within the gaming app where my model will have to return the prediction for new data (test set). For example, if the user enters the gaming app, plays and we want to return a prediction before the next level unlocks, I assume that the features available in this dataset are all available before the next level unlocks. In any case the dimensions of train and test the datasets match (with the same columns), so I assume the features used in the train set match the prediction point of the test set.\n",
    "\n",
    "I'll further split the train set into train and validation, to assess first of all the performance of the model on the validation set and use the latter for tuning eventual hyperparameters. I'll leave the test set untouched, and I'll address it as new data, meaning the test set will go through relevant transformations or preprocessing based on the train set before inputting it into the tuned model for final predictions.\n",
    "\n",
    "Finally, I'll evaluate the tuned model with the chosen features on the test set by comparing the RMSE and other relevant metrics between different learners."
   ],
   "id": "30a0ebffe24080a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:42:29.953600Z",
     "start_time": "2024-12-26T21:42:29.950261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#check if we have the same columns in train and test\n",
    "# Get the column sets\n",
    "train_columns = set(df_train.columns)\n",
    "test_columns = set(df_test.columns)\n",
    "\n",
    "# Check for equality\n",
    "if train_columns == test_columns:\n",
    "    print(\"The columns in df_train and df_test are the same.\")\n",
    "else:\n",
    "    print(\"The columns in df_train and df_test are different.\")\n",
    "\n",
    "    # Find columns in df_train but not in df_test\n",
    "    missing_in_test = train_columns - test_columns\n",
    "    if missing_in_test:\n",
    "        print(f\"Columns in df_train but not in df_test: {missing_in_test}\")\n",
    "\n",
    "    # Find columns in df_test but not in df_train\n",
    "    missing_in_train = test_columns - train_columns\n",
    "    if missing_in_train:\n",
    "        print(f\"Columns in df_test but not in df_train: {missing_in_train}\")"
   ],
   "id": "52c404544ee0684f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns in df_train and df_test are the same.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "EDA\n",
    "\n",
    "In this section I use sweetviz, a module that returns a html with a comprehensive exploratory data analysis including:\n",
    "- marginal and joint distributions of Y, continuous features and categorical features\n",
    "- measures of associations\n",
    "- the goal here is to:\n",
    "    - check eventual missing values (for imputation)\n",
    "    - check statistical outliers for capping or removal\n",
    "    - look at the correlations:\n",
    "        - with Y, to have a first glimpse of the predictive power of the feature (we ideally want features in X being highly correlated with Y in its absolute value).\n",
    "        - between features, to get an idea if we need to introduce interaction variables for highly correlated explanatory variables (ideally we want explanatory features independent between each other, so we do not want multicollinearity)\n",
    "    - check if we have constant features, which having 0 variance are not explanatory at all\n",
    "    - the following checks will help us chose whether we are in a linear or non-linear setting, and therefore the choice of the learners\n",
    "        - check the skewness of the label to assess if a transformation is needed\n",
    "        - check the scale of the features and the label.\n",
    "        - check the relationship between Y and the features\n",
    "    - check cardinality of categorical features for binning or one-hot-encoding\n",
    "    - check if we have duplicate rows"
   ],
   "id": "ce838e88740e396"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:50.222898Z",
     "start_time": "2024-12-26T21:42:29.973752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify data types. everything that is count, total, occurrence, price, number of days - I address it as continuous\n",
    "for col in df_train.columns:\n",
    "    if col in ['weekday', 'village_id']:\n",
    "        df_train[col] = df_train[col].astype('category')\n",
    "    else:\n",
    "        df_train[col] = df_train[col].astype(float)\n",
    "\n",
    "\n",
    "report = sv.analyze([df_train, \"Train\"], target_feat=\"org_price_usd_following_30_days\")\n",
    "report.show_html(\"regression_report.html\")"
   ],
   "id": "badf0ce77f3894a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done! Use 'show' commands to display/save.   |██████████| [100%]   00:07 -> (00:00 left)                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report regression_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:52.299699Z",
     "start_time": "2024-12-26T21:43:50.481360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# further check correlations\n",
    "correlation_matrix = df_train.corr()\n",
    "print(correlation_matrix['org_price_usd_following_30_days'].sort_values(ascending=False))"
   ],
   "id": "ac48f4af9f721ca9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org_price_usd_following_30_days                                1.000000\n",
      "org_price_usd_preceding_30_days                                0.728005\n",
      "spins_reward_preceding_30_days                                 0.716380\n",
      "pet_xp_reward_preceding_30_days                                0.689277\n",
      "org_price_usd_preceding_7_to_30_days                           0.675280\n",
      "org_price_usd_triple_preceding_30_days                         0.633510\n",
      "payment_occurrences_preceding_30_days                          0.619148\n",
      "tournament_spins_reward_7_preceding                            0.612171\n",
      "org_price_usd_preceding_3_days                                 0.583488\n",
      "org_price_usd_preceding_3_to_7_days                            0.577685\n",
      "payment_occurrences_preceding_7_to_30_days                     0.568057\n",
      "chests_reward_preceding_30_days                                0.508032\n",
      "payment_occurrences_preceding_3_days                           0.458481\n",
      "payment_occurrences_preceding_3_to_7_days                      0.452433\n",
      "tournament_chest_reward_7_preceding                            0.274798\n",
      "total_pets_feed_preceding_7_days                               0.266364\n",
      "avg_past_seen_price_oos_preceding_7_days                       0.255087\n",
      "tournament_number_of_rank_in_top_10_7_preceding                0.224986\n",
      "ltv_gross_up_to_preceding_30_days                              0.220045\n",
      "total_spins_preceding_7_days                                   0.171660\n",
      "total_raids_preceding_7_days                                   0.168987\n",
      "total_attacks_preceding_7_days                                 0.163449\n",
      "activity_occurrences_preceding_7_days                          0.158202\n",
      "village_id                                                     0.151502\n",
      "total_villages_completed_preceding_7_days                      0.091171\n",
      "total_oos_action_fire_preceding_7_days                         0.088311\n",
      "total_set_completed_preceding_7_days                           0.086450\n",
      "total_cards_collected_preceding_7_days                         0.085794\n",
      "total_card_xp_diff_preceding_7_days                            0.085665\n",
      "active_days_preceding_7_days                                   0.079151\n",
      "total_card_xp                                                  0.078008\n",
      "total_raids_on_user_preceding_7_days                           0.069971\n",
      "tournament_coins_reward_7_preceding                            0.064524\n",
      "raids_amount_normalized_preceding_7_days                       0.062804\n",
      "total_new_cards_collected_chest_from_store_preceding_7_days    0.053312\n",
      "active_days_preceding_7_to_14_days                             0.045272\n",
      "avg_spin_aggressiveness_oos_preceding_7_days                   0.038265\n",
      "total_oos_action_fire                                          0.030091\n",
      "total_attacks_on_user_preceding_7_days                         0.029826\n",
      "total_friend_link_invites_preceding_7_days                     0.029099\n",
      "chests_amount_normalized_preceding_7_days                      0.022657\n",
      "total_villages_completed                                       0.021229\n",
      "hours_since_installed_ma                                       0.020791\n",
      "spins_inventory_preceding_30_days                              0.017217\n",
      "total_payers_friends_active_in_the_last_7days                  0.016817\n",
      "total_cards_collected_inbox_preceding_7_days                   0.016591\n",
      "total_set_completed                                            0.009643\n",
      "total_friend_link_invites                                      0.007005\n",
      "total_friends_active_in_the_last_7days                        -0.000907\n",
      "weekday                                                       -0.005283\n",
      "total_repairs_preceding_7_days                                -0.008685\n",
      "total_friends_active_in_the_last_7_to_14_days                 -0.009144\n",
      "hours_in_village                                              -0.027576\n",
      "spins_rewards_lo_preceding_7_days                                   NaN\n",
      "Name: org_price_usd_following_30_days, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have some features that are highly correlated with Y, but that are highly correlated with each other. we address them later on. Fo example:",
   "id": "ace1041e5adf0a4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:52.327481Z",
     "start_time": "2024-12-26T21:43:52.306922Z"
    }
   },
   "cell_type": "code",
   "source": "df_train[['org_price_usd_preceding_30_days','spins_reward_preceding_30_days']].corr()",
   "id": "84de289eedf1a0f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                 org_price_usd_preceding_30_days  \\\n",
       "org_price_usd_preceding_30_days                         1.000000   \n",
       "spins_reward_preceding_30_days                          0.973278   \n",
       "\n",
       "                                 spins_reward_preceding_30_days  \n",
       "org_price_usd_preceding_30_days                        0.973278  \n",
       "spins_reward_preceding_30_days                         1.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_price_usd_preceding_30_days</th>\n",
       "      <th>spins_reward_preceding_30_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>org_price_usd_preceding_30_days</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spins_reward_preceding_30_days</th>\n",
       "      <td>0.973278</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Observations from EDA (in case the html does not work, I added the pictures_EDA folder with relevant screenshots):\n",
    "\n",
    "1. The Label, in black: first of all, it is very skewed (43.3% are zeros). And this is reflected by the skewness of 22.5. With a skewness this high, a transformation like log or sqrt will decrease the skewness but won't make the distribution symmetrical, which means that maybe a linear setting might not be ideal.\n",
    "2. For each feature, the report gives the following indexes to measure correlations:\n",
    "\n",
    "    a. Theil's U uncertainty coefficient: between categorical variables\n",
    "\n",
    "        - values are very low - which means that the features are independent.\n",
    "\n",
    "    b. Correlation Ratio (chi squared): between categorical and numerical variables\n",
    "\n",
    "        - the categorical variables (village_id, weekday) show a mild-weak association with Y.\n",
    "\n",
    "    c. Pearson Correlation: between numerical variables\n",
    "\n",
    "        - we can see that there are 2 features which are highly correlated with the label: org_price_usd_preceding_30_days (0.73), spins_reward_preceding_30_days (0.72).\n",
    "        - below them, 9 more features with moderate correlation (between 0.5 and 0.7)\n",
    "        - we do have multicolinearity\n",
    "\n",
    "3. A short explanation about the plots: each feature is depicted with both a histogram (feature values on the x axis, frequency (%) of feature value on the left vertical axis) and a line plot with aggregated data (feature values on the x axis, average Y value on the right vertical axis). We can see that many of the features present a non-linear relationship with Y.\n",
    "\n",
    "\n",
    "ACTION ITEMS\n",
    "- drop duplicates rows\n",
    "- drop constant feature: spins_rewards_lo_preceding_7_days\n",
    "- There are some features that sweetviz addressed as categorical: ['active_days_preceding_7_days', 'active_days_preceding_7_to_14_days', 'total_set_completed', 'total_friend_link_invites']. I am not sure they are, so I computed for them the pearson correlation which still resulted very low. I address these features as numerical.\n",
    "- village_id and weekday: I'll bin them according to sweetviz suggestion into less categories considering the high cardinality and I apply OHE, leaving out the \"other\" category for redundancy.\n",
    "- we don't have missing values. so no imputation needed.\n",
    "- we have one feature with negative values: ltv_gross_up_to_preceding_30_days\n",
    "- address highly correlated features: features that are highly correlated with Y are used to generate interaction features. features that have lower correlations with Y are dropped.\n",
    "- scaling (for skewness): I'll apply log transformation and minmax scaling to numerical features\n",
    "- outliers: I'll cap outliers according to IQR\n",
    "- variance: I'll drop features whose variance is below a given threshold (0.01 and 0.1 for binary and numerical features accordingly).\n",
    "\n",
    "Considering the setting is non-linear, that features and label have different scales, that we might have outliers given the presence of high average Y values in the line plots, and considering the presence of highly correlated features, it seems like tree-based learners would be a better fit for this problem. Tree based learners are robust to outliers and correlated features, and are scale indifferent - but I'm going to address this anyway."
   ],
   "id": "ecc154e0b3195e46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:52.455887Z",
     "start_time": "2024-12-26T21:43:52.367633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop constant features\n",
    "df_train.drop(columns=['spins_rewards_lo_preceding_7_days'], inplace=True)"
   ],
   "id": "150194922dd8a83b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:52.957604Z",
     "start_time": "2024-12-26T21:43:52.488513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop duplicates\n",
    "df_train.drop_duplicates(inplace=True)"
   ],
   "id": "279f7ad241e8691d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:53.088087Z",
     "start_time": "2024-12-26T21:43:52.997680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#split into train-dev set\n",
    "X = df_train.drop(columns=['org_price_usd_following_30_days'])\n",
    "y = df_train['org_price_usd_following_30_days']"
   ],
   "id": "a3ada7cc1246d1a6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:53.106566Z",
     "start_time": "2024-12-26T21:43:53.095629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_binning_and_encoding(df, target_column, top_n=None, categories_to_keep=None, fit=True, prefix=None):\n",
    "    \"\"\"\n",
    "    Generalized function for binning and encoding a categorical column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame to preprocess.\n",
    "    - target_column (str): The column to bin and encode.\n",
    "    - top_n (int): Number of top categories to keep based on frequency (fit phase uses this to determine categories_to_keep).\n",
    "                   Ignored if categories_to_keep is provided.\n",
    "    - categories_to_keep (list): List of categories to keep (if provided, fit mode skips determining categories).\n",
    "    - fit (bool): Whether to determine categories_to_keep (True for training dataset, False for dev/test dataset).\n",
    "    - prefix (str): Prefix for created columns during one-hot encoding. Defaults to the target_column name.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The processed DataFrame with the binned and encoded column.\n",
    "    - categories_to_keep (list): The list of categories kept during the fit process (if fit=True).\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Work on a copy to prevent changes to the original DataFrame\n",
    "\n",
    "    # Step 1: Convert the target column to string if not already\n",
    "    df[target_column] = df[target_column].astype(int).astype(str)\n",
    "\n",
    "    # Step 2: Determine the categories to keep (fit phase)\n",
    "    if fit:\n",
    "        if categories_to_keep is None:\n",
    "            # Automatically determine top categories by frequency\n",
    "            categories_to_keep = df[target_column].value_counts().index.tolist()[:top_n]\n",
    "\n",
    "    # Step 3: Bin the target column\n",
    "    binned_column = f\"{target_column}_binned\"\n",
    "    df[binned_column] = df[target_column].apply(\n",
    "        lambda x: x if x in categories_to_keep else 'Other'\n",
    "    )\n",
    "\n",
    "    # Step 4: One-hot encode the binned column\n",
    "    prefix = prefix if prefix else target_column\n",
    "    df_encoded = pd.get_dummies(df, columns=[binned_column], prefix=prefix)\n",
    "\n",
    "    # Step 5: Ensure consistent columns across datasets (fixed based on categories_to_keep)\n",
    "    fixed_columns = [f\"{prefix}_{cat}\" for cat in categories_to_keep]\n",
    "    for col in fixed_columns:\n",
    "        if col not in df_encoded:\n",
    "            df_encoded[col] = 0  # Add missing columns with default value 0\n",
    "\n",
    "    # Remove unnecessary columns (like 'Other')\n",
    "    extra_columns = [col for col in df_encoded.columns if col.startswith(f\"{prefix}_\") and col not in fixed_columns]\n",
    "    df_encoded = df_encoded.drop(columns=extra_columns)\n",
    "\n",
    "    # Step 6: Drop the original target column\n",
    "    df_encoded = df_encoded.drop(columns=[target_column, binned_column], errors=\"ignore\")\n",
    "\n",
    "    # Step 7: Reorder the one-hot encoded columns\n",
    "    df_encoded = df_encoded[fixed_columns]\n",
    "\n",
    "    # Step 8: Merge back with the original DataFrame (other columns untouched)\n",
    "    df = pd.concat([df.drop(columns=[target_column, binned_column], errors=\"ignore\"), df_encoded], axis=1)\n",
    "\n",
    "    # Return the processed DataFrame and, if in fit mode, the categories_to_keep\n",
    "    if fit:\n",
    "        return df, categories_to_keep\n",
    "    else:\n",
    "        return df"
   ],
   "id": "66d577381dfaf7c7",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:53.145521Z",
     "start_time": "2024-12-26T21:43:53.139463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LogTransformer:\n",
    "    def __init__(self):\n",
    "        # List of features to transform\n",
    "        self.features = None\n",
    "        # Dictionary to store shift values for each feature\n",
    "        self.shifts = {}\n",
    "\n",
    "    def fit(self, df, features):\n",
    "        \"\"\"\n",
    "        Check and store the features that will be log-transformed.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The training dataset\n",
    "        - features (list): Non-binary numerical features for log transformation\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        # Calculate and store the shift value for each feature (if needed)\n",
    "        for feature in features:\n",
    "            min_value = df[feature].min()\n",
    "            self.shifts[feature] = abs(min_value) + 1 if min_value < 0 else 0\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Apply the log transformation to the stored features.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The dataset to transform\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Dataset with log-transformed features\n",
    "        \"\"\"\n",
    "        df = df.copy()  # Avoid modifying the original DataFrame\n",
    "        if self.features is None:\n",
    "            raise ValueError(\"You need to fit the transformer before applying transform.\")\n",
    "        # Apply log transformation with shifting (if necessary)\n",
    "        for feature in self.features:\n",
    "            shift_value = self.shifts.get(feature, 0)\n",
    "            df[feature] = np.log1p(df[feature] + shift_value)\n",
    "        return df"
   ],
   "id": "9ba059eeacdccfb2",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:53.159820Z",
     "start_time": "2024-12-26T21:43:53.154680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Outlier capping with IQR - Generalized for Fit/Transform\n",
    "class OutlierCapper:\n",
    "    def __init__(self):\n",
    "        # Dictionary to store the lower and upper bounds for each feature\n",
    "        self.bounds_ = {}\n",
    "\n",
    "    def fit(self, df, features):\n",
    "        \"\"\"\n",
    "        Compute the IQR-based outlier bounds for the given features.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The training dataset\n",
    "        - features (list): List of feature names to compute bounds for\n",
    "\n",
    "        Returns:\n",
    "        - None: Saves bounds to self.bounds_\n",
    "        \"\"\"\n",
    "        for feature in features:\n",
    "            # Compute Q1 (25th percentile) and Q3 (75th percentile)\n",
    "            Q1 = df[feature].quantile(0.25)\n",
    "            Q3 = df[feature].quantile(0.75)\n",
    "            # Compute Interquartile Range (IQR)\n",
    "            IQR = Q3 - Q1\n",
    "            # Define lower and upper bounds for outlier capping\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            # Store the bounds for the feature\n",
    "            self.bounds_[feature] = (lower_bound, upper_bound)\n",
    "\n",
    "    def transform(self, df, features):\n",
    "        \"\"\"\n",
    "        Apply the precomputed bounds to cap outliers in the DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The dataset to apply the bounds to\n",
    "        - features (list): List of feature names to apply bounds to\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Dataset with capped values\n",
    "        \"\"\"\n",
    "        df = df.copy()  # Avoid modifying the original dataset\n",
    "        for feature in features:\n",
    "            if feature in self.bounds_:\n",
    "                lower_bound, upper_bound = self.bounds_[feature]\n",
    "                # Cap the outliers to the precomputed bounds\n",
    "                df[feature] = np.where(df[feature] < lower_bound, lower_bound, df[feature])\n",
    "                df[feature] = np.where(df[feature] > upper_bound, upper_bound, df[feature])\n",
    "        return df"
   ],
   "id": "d7c7a3078b5c48a0",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Further split train into train-dev",
   "id": "dba8645824637067"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:53.318555Z",
     "start_time": "2024-12-26T21:43:53.164716Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=42)",
   "id": "f7a8619f8b96b449",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:54.180040Z",
     "start_time": "2024-12-26T21:43:53.322492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For village_id, keep the top 13 categories\n",
    "X_train_processed, village_ids_to_keep = preprocess_binning_and_encoding(\n",
    "    df=X_train,\n",
    "    target_column=\"village_id\",\n",
    "    top_n=13,\n",
    "    fit=True\n",
    ")\n",
    "# For weekday, keep top 4 categories\n",
    "X_train_processed, weekdays_to_keep = preprocess_binning_and_encoding(\n",
    "    df=X_train_processed,\n",
    "    target_column=\"weekday\",\n",
    "    top_n=4,\n",
    "    fit=True\n",
    ")"
   ],
   "id": "494094fabbfcbd78",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:54.407602Z",
     "start_time": "2024-12-26T21:43:54.186023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess dev set for village_id\n",
    "X_dev_processed = preprocess_binning_and_encoding(\n",
    "    df=X_dev,\n",
    "    target_column=\"village_id\",\n",
    "    categories_to_keep=village_ids_to_keep,\n",
    "    fit=False\n",
    ")\n",
    "\n",
    "# Preprocess dev set for weekday\n",
    "X_dev_processed = preprocess_binning_and_encoding(\n",
    "    df=X_dev_processed,\n",
    "    target_column=\"weekday\",\n",
    "    categories_to_keep=weekdays_to_keep,\n",
    "    fit=False\n",
    ")"
   ],
   "id": "fbf5b7bfa6b683a8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:54.466114Z",
     "start_time": "2024-12-26T21:43:54.411705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Recompute numerical features from the processed DataFrame\n",
    "numerical_features = X_train_processed.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Binary features from one-hot encoding (they shouldn't be capped)\n",
    "binary_features = [\n",
    "    col for col in X_train_processed.columns\n",
    "    if col.startswith('weekday_') or col.startswith('village_id_')\n",
    "]"
   ],
   "id": "6267403187488afd",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:54.733623Z",
     "start_time": "2024-12-26T21:43:54.470572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize LogTransformer, addressing columns with negative values\n",
    "log_transformer = LogTransformer()\n",
    "\n",
    "# Fit log transformer on X_train_cleaned\n",
    "log_transformer.fit(X_train_processed, numerical_features)\n",
    "\n",
    "# Apply log transformation to train, dev, and test datasets\n",
    "X_train_log_transformed = log_transformer.transform(X_train_processed)\n",
    "X_dev_log_transformed = log_transformer.transform(X_dev_processed)"
   ],
   "id": "698dc70f92d77a49",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:55.317026Z",
     "start_time": "2024-12-26T21:43:54.741385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize OutlierCapper\n",
    "outlier_capper = OutlierCapper()\n",
    "\n",
    "# Fit the capper on the training set\n",
    "outlier_capper.fit(X_train_log_transformed, numerical_features)\n",
    "\n",
    "# Transform train, dev, and test sets\n",
    "X_train_cleaned = outlier_capper.transform(X_train_log_transformed, numerical_features)\n",
    "X_dev_cleaned = outlier_capper.transform(X_dev_log_transformed, numerical_features)"
   ],
   "id": "8bb108633be3a81f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:55.691332Z",
     "start_time": "2024-12-26T21:43:55.321808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Initialize separate VarianceThreshold selectors\n",
    "binary_selector = VarianceThreshold(threshold=0.01)  # For binary features\n",
    "numerical_selector = VarianceThreshold(threshold=0.1)  # For numerical features\n",
    "\n",
    "# Step 2: Fit selectors on the training data\n",
    "binary_selector.fit(X_train_log_transformed[binary_features])  # Fit on binary features\n",
    "numerical_selector.fit(X_train_log_transformed[numerical_features])  # Fit on numerical features\n",
    "\n",
    "final_selected_features = (\n",
    "        list(pd.Index(binary_features)[binary_selector.get_support()]) +  # Selected binary features\n",
    "        list(pd.Index(numerical_features)[numerical_selector.get_support()])  # Selected numerical features\n",
    ")\n",
    "\n",
    "if len(final_selected_features) == 0:\n",
    "    raise ValueError(\"No features were selected. Consider lowering the variance thresholds.\")\n",
    "\n",
    "# Step 4: Transform train, dev, and test sets\n",
    "X_train_selected = X_train_log_transformed[final_selected_features]  # Only selected features from train set\n",
    "X_dev_selected = X_dev_log_transformed[final_selected_features]  # Subset dev set based on `final_selected_features`\n",
    "\n",
    "# Print selected features\n",
    "print(f\"Selected Features ({len(final_selected_features)} in total):\")\n",
    "print(final_selected_features)"
   ],
   "id": "6695643aa135d365",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (61 in total):\n",
      "['village_id_0', 'village_id_170', 'village_id_165', 'village_id_168', 'village_id_166', 'village_id_160', 'village_id_169', 'village_id_175', 'village_id_167', 'village_id_172', 'village_id_161', 'village_id_171', 'village_id_162', 'weekday_4', 'weekday_3', 'weekday_5', 'weekday_2', 'payment_occurrences_preceding_30_days', 'hours_since_installed_ma', 'total_raids_preceding_7_days', 'org_price_usd_preceding_3_days', 'hours_in_village', 'total_card_xp', 'total_villages_completed_preceding_7_days', 'total_friends_active_in_the_last_7_to_14_days', 'chests_reward_preceding_30_days', 'tournament_number_of_rank_in_top_10_7_preceding', 'pet_xp_reward_preceding_30_days', 'total_set_completed_preceding_7_days', 'total_oos_action_fire_preceding_7_days', 'total_spins_preceding_7_days', 'total_card_xp_diff_preceding_7_days', 'tournament_spins_reward_7_preceding', 'avg_spin_aggressiveness_oos_preceding_7_days', 'total_pets_feed_preceding_7_days', 'active_days_preceding_7_to_14_days', 'total_cards_collected_preceding_7_days', 'total_friend_link_invites_preceding_7_days', 'total_friends_active_in_the_last_7days', 'payment_occurrences_preceding_3_days', 'payment_occurrences_preceding_7_to_30_days', 'tournament_chest_reward_7_preceding', 'total_new_cards_collected_chest_from_store_preceding_7_days', 'raids_amount_normalized_preceding_7_days', 'payment_occurrences_preceding_3_to_7_days', 'total_raids_on_user_preceding_7_days', 'org_price_usd_preceding_7_to_30_days', 'avg_past_seen_price_oos_preceding_7_days', 'total_cards_collected_inbox_preceding_7_days', 'chests_amount_normalized_preceding_7_days', 'total_repairs_preceding_7_days', 'org_price_usd_preceding_30_days', 'total_attacks_preceding_7_days', 'org_price_usd_triple_preceding_30_days', 'total_attacks_on_user_preceding_7_days', 'activity_occurrences_preceding_7_days', 'total_payers_friends_active_in_the_last_7days', 'spins_inventory_preceding_30_days', 'org_price_usd_preceding_3_to_7_days', 'spins_reward_preceding_30_days', 'active_days_preceding_7_days']\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:55.703926Z",
     "start_time": "2024-12-26T21:43:55.697883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify unselected features for binary features\n",
    "binary_unselected_features = [\n",
    "    column for i, column in enumerate(binary_features)\n",
    "    if not binary_selector.get_support()[i]\n",
    "]\n",
    "\n",
    "# Identify unselected features for numerical features\n",
    "numerical_unselected_features = [\n",
    "    column for i, column in enumerate(numerical_features)\n",
    "    if not numerical_selector.get_support()[i]\n",
    "]\n",
    "\n",
    "# Combine unselected features\n",
    "unselected_features = binary_unselected_features + numerical_unselected_features\n",
    "\n",
    "# Print the unselected features along with their count\n",
    "print(f\"Unselected Features ({len(unselected_features)} in total):\")\n",
    "print(unselected_features)"
   ],
   "id": "96d5d5a5011c4017",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unselected Features (6 in total):\n",
      "['total_villages_completed', 'ltv_gross_up_to_preceding_30_days', 'total_friend_link_invites', 'tournament_coins_reward_7_preceding', 'total_set_completed', 'total_oos_action_fire']\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:55.738111Z",
     "start_time": "2024-12-26T21:43:55.733064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# handle multicollinearity by generating interaction terms, and dropping features less correlated to Y\n",
    "\n",
    "def generate_interaction_terms(df):\n",
    "\n",
    "    df['interaction_org_price_usd_spin_rewards_preceding_30_days'] = df['org_price_usd_preceding_30_days'] * df['spins_reward_preceding_30_days'] * df['payment_occurrences_preceding_30_days']\n",
    "\n",
    "    df['interaction_preceding_3_days'] = df['payment_occurrences_preceding_3_days'] * df['org_price_usd_preceding_3_days']\n",
    "\n",
    "    df.drop(columns=['org_price_usd_preceding_30_days', 'spins_reward_preceding_30_days', 'payment_occurrences_preceding_30_days', 'payment_occurrences_preceding_3_days', 'org_price_usd_preceding_3_days', 'org_price_usd_preceding_7_to_30_days', 'payment_occurrences_preceding_7_to_30_days'], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "d4df79eaa060ed90",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:55.845549Z",
     "start_time": "2024-12-26T21:43:55.752313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_selected = generate_interaction_terms(df=X_train_selected)\n",
    "X_dev_selected = generate_interaction_terms(df=X_dev_selected)"
   ],
   "id": "f898c7a245860676",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tz/4zg0f6x90m7cbq4q1p10p1m40000gn/T/ipykernel_36209/2653920461.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['interaction_org_price_usd_spin_rewards_preceding_30_days'] = df['org_price_usd_preceding_30_days'] * df['spins_reward_preceding_30_days'] * df['payment_occurrences_preceding_30_days']\n",
      "/var/folders/tz/4zg0f6x90m7cbq4q1p10p1m40000gn/T/ipykernel_36209/2653920461.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['interaction_preceding_3_days'] = df['payment_occurrences_preceding_3_days'] * df['org_price_usd_preceding_3_days']\n",
      "/var/folders/tz/4zg0f6x90m7cbq4q1p10p1m40000gn/T/ipykernel_36209/2653920461.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=['org_price_usd_preceding_30_days', 'spins_reward_preceding_30_days', 'payment_occurrences_preceding_30_days', 'payment_occurrences_preceding_3_days', 'org_price_usd_preceding_3_days', 'org_price_usd_preceding_7_to_30_days', 'payment_occurrences_preceding_7_to_30_days'], inplace=True)\n",
      "/var/folders/tz/4zg0f6x90m7cbq4q1p10p1m40000gn/T/ipykernel_36209/2653920461.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['interaction_org_price_usd_spin_rewards_preceding_30_days'] = df['org_price_usd_preceding_30_days'] * df['spins_reward_preceding_30_days'] * df['payment_occurrences_preceding_30_days']\n",
      "/var/folders/tz/4zg0f6x90m7cbq4q1p10p1m40000gn/T/ipykernel_36209/2653920461.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['interaction_preceding_3_days'] = df['payment_occurrences_preceding_3_days'] * df['org_price_usd_preceding_3_days']\n",
      "/var/folders/tz/4zg0f6x90m7cbq4q1p10p1m40000gn/T/ipykernel_36209/2653920461.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=['org_price_usd_preceding_30_days', 'spins_reward_preceding_30_days', 'payment_occurrences_preceding_30_days', 'payment_occurrences_preceding_3_days', 'org_price_usd_preceding_3_days', 'org_price_usd_preceding_7_to_30_days', 'payment_occurrences_preceding_7_to_30_days'], inplace=True)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:56.245638Z",
     "start_time": "2024-12-26T21:43:55.857416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Initialize the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))  # Adjust range if needed\n",
    "\n",
    "# Step 2: Separate numerical features (non-binary) for scaling\n",
    "numerical_to_scale = [feature for feature in numerical_features if feature in X_train_selected.columns]\n",
    "\n",
    "# Step 3: Fit the scaler on the numerical features of the training set\n",
    "X_train_selected_numerical_scaled = X_train_selected.copy()  # Copy to avoid modifying the original data\n",
    "X_train_selected_numerical_scaled[numerical_to_scale] = scaler.fit_transform(X_train_selected[numerical_to_scale])\n",
    "\n",
    "# Step 4: Apply the scaler to dev and test sets\n",
    "X_dev_selected_scaled = X_dev_selected.copy()\n",
    "X_dev_selected_scaled[numerical_to_scale] = scaler.transform(X_dev_selected[numerical_to_scale])"
   ],
   "id": "6ec93af73fa2c8a3",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:43:56.365383Z",
     "start_time": "2024-12-26T21:43:56.249913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = X_train_selected_numerical_scaled.copy()\n",
    "X_dev = X_dev_selected_scaled.copy()"
   ],
   "id": "86c410802156553a",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tree based feature importance",
   "id": "c7fe3d49d49d7ba6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:45:05.950413Z",
     "start_time": "2024-12-26T21:43:56.369329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "importances = gb.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "cumulative_importance = np.cumsum(importances[sorted_indices])\n",
    "threshold_index = np.where(cumulative_importance >= 0.95)[0][0]\n",
    "selected_features = X_train.columns[sorted_indices[:threshold_index + 1]]"
   ],
   "id": "3ee72c658bcddac0",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:45:05.975249Z",
     "start_time": "2024-12-26T21:45:05.971373Z"
    }
   },
   "cell_type": "code",
   "source": "len(selected_features)",
   "id": "35354bcecaf6e6ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:45:06.037529Z",
     "start_time": "2024-12-26T21:45:06.034755Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Selected Features (95% cumulative importance): {list(selected_features)}\")",
   "id": "8a869df72f7618d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (95% cumulative importance): ['interaction_org_price_usd_spin_rewards_preceding_30_days', 'tournament_spins_reward_7_preceding', 'org_price_usd_triple_preceding_30_days', 'interaction_preceding_3_days', 'pet_xp_reward_preceding_30_days', 'org_price_usd_preceding_3_to_7_days', 'hours_since_installed_ma', 'chests_reward_preceding_30_days', 'total_spins_preceding_7_days', 'total_villages_completed_preceding_7_days', 'total_card_xp', 'avg_past_seen_price_oos_preceding_7_days']\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:45:06.135898Z",
     "start_time": "2024-12-26T21:45:06.060876Z"
    }
   },
   "cell_type": "code",
   "source": "X_train[selected_features].corr()",
   "id": "45fd9262ef623e23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                    interaction_org_price_usd_spin_rewards_preceding_30_days  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                                           1.000000          \n",
       "tournament_spins_reward_7_preceding                                                          0.391211          \n",
       "org_price_usd_triple_preceding_30_days                                                       0.757776          \n",
       "interaction_preceding_3_days                                                                 0.580668          \n",
       "pet_xp_reward_preceding_30_days                                                              0.518192          \n",
       "org_price_usd_preceding_3_to_7_days                                                          0.605447          \n",
       "hours_since_installed_ma                                                                     0.058506          \n",
       "chests_reward_preceding_30_days                                                              0.810140          \n",
       "total_spins_preceding_7_days                                                                 0.187956          \n",
       "total_villages_completed_preceding_7_days                                                    0.061099          \n",
       "total_card_xp                                                                                0.083262          \n",
       "avg_past_seen_price_oos_preceding_7_days                                                     0.296680          \n",
       "\n",
       "                                                    tournament_spins_reward_7_preceding  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                             0.391211   \n",
       "tournament_spins_reward_7_preceding                                            1.000000   \n",
       "org_price_usd_triple_preceding_30_days                                         0.344838   \n",
       "interaction_preceding_3_days                                                   0.411001   \n",
       "pet_xp_reward_preceding_30_days                                                0.280371   \n",
       "org_price_usd_preceding_3_to_7_days                                            0.514908   \n",
       "hours_since_installed_ma                                                       0.073770   \n",
       "chests_reward_preceding_30_days                                                0.300493   \n",
       "total_spins_preceding_7_days                                                   0.368017   \n",
       "total_villages_completed_preceding_7_days                                      0.238756   \n",
       "total_card_xp                                                                  0.092398   \n",
       "avg_past_seen_price_oos_preceding_7_days                                       0.311000   \n",
       "\n",
       "                                                    org_price_usd_triple_preceding_30_days  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                                0.757776   \n",
       "tournament_spins_reward_7_preceding                                               0.344838   \n",
       "org_price_usd_triple_preceding_30_days                                            1.000000   \n",
       "interaction_preceding_3_days                                                      0.395057   \n",
       "pet_xp_reward_preceding_30_days                                                   0.686086   \n",
       "org_price_usd_preceding_3_to_7_days                                               0.471256   \n",
       "hours_since_installed_ma                                                          0.266369   \n",
       "chests_reward_preceding_30_days                                                   0.538212   \n",
       "total_spins_preceding_7_days                                                      0.126432   \n",
       "total_villages_completed_preceding_7_days                                        -0.121117   \n",
       "total_card_xp                                                                     0.062466   \n",
       "avg_past_seen_price_oos_preceding_7_days                                          0.289122   \n",
       "\n",
       "                                                    interaction_preceding_3_days  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                      0.580668   \n",
       "tournament_spins_reward_7_preceding                                     0.411001   \n",
       "org_price_usd_triple_preceding_30_days                                  0.395057   \n",
       "interaction_preceding_3_days                                            1.000000   \n",
       "pet_xp_reward_preceding_30_days                                         0.265688   \n",
       "org_price_usd_preceding_3_to_7_days                                     0.367012   \n",
       "hours_since_installed_ma                                               -0.025045   \n",
       "chests_reward_preceding_30_days                                         0.447752   \n",
       "total_spins_preceding_7_days                                            0.221539   \n",
       "total_villages_completed_preceding_7_days                               0.236906   \n",
       "total_card_xp                                                           0.084992   \n",
       "avg_past_seen_price_oos_preceding_7_days                                0.218036   \n",
       "\n",
       "                                                    pet_xp_reward_preceding_30_days  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                         0.518192   \n",
       "tournament_spins_reward_7_preceding                                        0.280371   \n",
       "org_price_usd_triple_preceding_30_days                                     0.686086   \n",
       "interaction_preceding_3_days                                               0.265688   \n",
       "pet_xp_reward_preceding_30_days                                            1.000000   \n",
       "org_price_usd_preceding_3_to_7_days                                        0.325781   \n",
       "hours_since_installed_ma                                                   0.229785   \n",
       "chests_reward_preceding_30_days                                            0.351481   \n",
       "total_spins_preceding_7_days                                               0.129724   \n",
       "total_villages_completed_preceding_7_days                                 -0.064288   \n",
       "total_card_xp                                                              0.057428   \n",
       "avg_past_seen_price_oos_preceding_7_days                                   0.222487   \n",
       "\n",
       "                                                    org_price_usd_preceding_3_to_7_days  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                             0.605447   \n",
       "tournament_spins_reward_7_preceding                                            0.514908   \n",
       "org_price_usd_triple_preceding_30_days                                         0.471256   \n",
       "interaction_preceding_3_days                                                   0.367012   \n",
       "pet_xp_reward_preceding_30_days                                                0.325781   \n",
       "org_price_usd_preceding_3_to_7_days                                            1.000000   \n",
       "hours_since_installed_ma                                                       0.022655   \n",
       "chests_reward_preceding_30_days                                                0.452369   \n",
       "total_spins_preceding_7_days                                                   0.260433   \n",
       "total_villages_completed_preceding_7_days                                      0.224170   \n",
       "total_card_xp                                                                  0.060157   \n",
       "avg_past_seen_price_oos_preceding_7_days                                       0.285095   \n",
       "\n",
       "                                                    hours_since_installed_ma  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                  0.058506   \n",
       "tournament_spins_reward_7_preceding                                 0.073770   \n",
       "org_price_usd_triple_preceding_30_days                              0.266369   \n",
       "interaction_preceding_3_days                                       -0.025045   \n",
       "pet_xp_reward_preceding_30_days                                     0.229785   \n",
       "org_price_usd_preceding_3_to_7_days                                 0.022655   \n",
       "hours_since_installed_ma                                            1.000000   \n",
       "chests_reward_preceding_30_days                                     0.035472   \n",
       "total_spins_preceding_7_days                                       -0.120826   \n",
       "total_villages_completed_preceding_7_days                          -0.504087   \n",
       "total_card_xp                                                      -0.038457   \n",
       "avg_past_seen_price_oos_preceding_7_days                            0.047079   \n",
       "\n",
       "                                                    chests_reward_preceding_30_days  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                         0.810140   \n",
       "tournament_spins_reward_7_preceding                                        0.300493   \n",
       "org_price_usd_triple_preceding_30_days                                     0.538212   \n",
       "interaction_preceding_3_days                                               0.447752   \n",
       "pet_xp_reward_preceding_30_days                                            0.351481   \n",
       "org_price_usd_preceding_3_to_7_days                                        0.452369   \n",
       "hours_since_installed_ma                                                   0.035472   \n",
       "chests_reward_preceding_30_days                                            1.000000   \n",
       "total_spins_preceding_7_days                                               0.136834   \n",
       "total_villages_completed_preceding_7_days                                  0.026103   \n",
       "total_card_xp                                                              0.068902   \n",
       "avg_past_seen_price_oos_preceding_7_days                                   0.201500   \n",
       "\n",
       "                                                    total_spins_preceding_7_days  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                      0.187956   \n",
       "tournament_spins_reward_7_preceding                                     0.368017   \n",
       "org_price_usd_triple_preceding_30_days                                  0.126432   \n",
       "interaction_preceding_3_days                                            0.221539   \n",
       "pet_xp_reward_preceding_30_days                                         0.129724   \n",
       "org_price_usd_preceding_3_to_7_days                                     0.260433   \n",
       "hours_since_installed_ma                                               -0.120826   \n",
       "chests_reward_preceding_30_days                                         0.136834   \n",
       "total_spins_preceding_7_days                                            1.000000   \n",
       "total_villages_completed_preceding_7_days                               0.500875   \n",
       "total_card_xp                                                           0.208040   \n",
       "avg_past_seen_price_oos_preceding_7_days                                0.411977   \n",
       "\n",
       "                                                    total_villages_completed_preceding_7_days  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...                                   0.061099   \n",
       "tournament_spins_reward_7_preceding                                                  0.238756   \n",
       "org_price_usd_triple_preceding_30_days                                              -0.121117   \n",
       "interaction_preceding_3_days                                                         0.236906   \n",
       "pet_xp_reward_preceding_30_days                                                     -0.064288   \n",
       "org_price_usd_preceding_3_to_7_days                                                  0.224170   \n",
       "hours_since_installed_ma                                                            -0.504087   \n",
       "chests_reward_preceding_30_days                                                      0.026103   \n",
       "total_spins_preceding_7_days                                                         0.500875   \n",
       "total_villages_completed_preceding_7_days                                            1.000000   \n",
       "total_card_xp                                                                        0.109440   \n",
       "avg_past_seen_price_oos_preceding_7_days                                             0.183388   \n",
       "\n",
       "                                                    total_card_xp  \\\n",
       "interaction_org_price_usd_spin_rewards_precedin...       0.083262   \n",
       "tournament_spins_reward_7_preceding                      0.092398   \n",
       "org_price_usd_triple_preceding_30_days                   0.062466   \n",
       "interaction_preceding_3_days                             0.084992   \n",
       "pet_xp_reward_preceding_30_days                          0.057428   \n",
       "org_price_usd_preceding_3_to_7_days                      0.060157   \n",
       "hours_since_installed_ma                                -0.038457   \n",
       "chests_reward_preceding_30_days                          0.068902   \n",
       "total_spins_preceding_7_days                             0.208040   \n",
       "total_villages_completed_preceding_7_days                0.109440   \n",
       "total_card_xp                                            1.000000   \n",
       "avg_past_seen_price_oos_preceding_7_days                 0.088315   \n",
       "\n",
       "                                                    avg_past_seen_price_oos_preceding_7_days  \n",
       "interaction_org_price_usd_spin_rewards_precedin...                                  0.296680  \n",
       "tournament_spins_reward_7_preceding                                                 0.311000  \n",
       "org_price_usd_triple_preceding_30_days                                              0.289122  \n",
       "interaction_preceding_3_days                                                        0.218036  \n",
       "pet_xp_reward_preceding_30_days                                                     0.222487  \n",
       "org_price_usd_preceding_3_to_7_days                                                 0.285095  \n",
       "hours_since_installed_ma                                                            0.047079  \n",
       "chests_reward_preceding_30_days                                                     0.201500  \n",
       "total_spins_preceding_7_days                                                        0.411977  \n",
       "total_villages_completed_preceding_7_days                                           0.183388  \n",
       "total_card_xp                                                                       0.088315  \n",
       "avg_past_seen_price_oos_preceding_7_days                                            1.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interaction_org_price_usd_spin_rewards_preceding_30_days</th>\n",
       "      <th>tournament_spins_reward_7_preceding</th>\n",
       "      <th>org_price_usd_triple_preceding_30_days</th>\n",
       "      <th>interaction_preceding_3_days</th>\n",
       "      <th>pet_xp_reward_preceding_30_days</th>\n",
       "      <th>org_price_usd_preceding_3_to_7_days</th>\n",
       "      <th>hours_since_installed_ma</th>\n",
       "      <th>chests_reward_preceding_30_days</th>\n",
       "      <th>total_spins_preceding_7_days</th>\n",
       "      <th>total_villages_completed_preceding_7_days</th>\n",
       "      <th>total_card_xp</th>\n",
       "      <th>avg_past_seen_price_oos_preceding_7_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>interaction_org_price_usd_spin_rewards_preceding_30_days</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391211</td>\n",
       "      <td>0.757776</td>\n",
       "      <td>0.580668</td>\n",
       "      <td>0.518192</td>\n",
       "      <td>0.605447</td>\n",
       "      <td>0.058506</td>\n",
       "      <td>0.810140</td>\n",
       "      <td>0.187956</td>\n",
       "      <td>0.061099</td>\n",
       "      <td>0.083262</td>\n",
       "      <td>0.296680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tournament_spins_reward_7_preceding</th>\n",
       "      <td>0.391211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.344838</td>\n",
       "      <td>0.411001</td>\n",
       "      <td>0.280371</td>\n",
       "      <td>0.514908</td>\n",
       "      <td>0.073770</td>\n",
       "      <td>0.300493</td>\n",
       "      <td>0.368017</td>\n",
       "      <td>0.238756</td>\n",
       "      <td>0.092398</td>\n",
       "      <td>0.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org_price_usd_triple_preceding_30_days</th>\n",
       "      <td>0.757776</td>\n",
       "      <td>0.344838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395057</td>\n",
       "      <td>0.686086</td>\n",
       "      <td>0.471256</td>\n",
       "      <td>0.266369</td>\n",
       "      <td>0.538212</td>\n",
       "      <td>0.126432</td>\n",
       "      <td>-0.121117</td>\n",
       "      <td>0.062466</td>\n",
       "      <td>0.289122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interaction_preceding_3_days</th>\n",
       "      <td>0.580668</td>\n",
       "      <td>0.411001</td>\n",
       "      <td>0.395057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265688</td>\n",
       "      <td>0.367012</td>\n",
       "      <td>-0.025045</td>\n",
       "      <td>0.447752</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>0.236906</td>\n",
       "      <td>0.084992</td>\n",
       "      <td>0.218036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet_xp_reward_preceding_30_days</th>\n",
       "      <td>0.518192</td>\n",
       "      <td>0.280371</td>\n",
       "      <td>0.686086</td>\n",
       "      <td>0.265688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.325781</td>\n",
       "      <td>0.229785</td>\n",
       "      <td>0.351481</td>\n",
       "      <td>0.129724</td>\n",
       "      <td>-0.064288</td>\n",
       "      <td>0.057428</td>\n",
       "      <td>0.222487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org_price_usd_preceding_3_to_7_days</th>\n",
       "      <td>0.605447</td>\n",
       "      <td>0.514908</td>\n",
       "      <td>0.471256</td>\n",
       "      <td>0.367012</td>\n",
       "      <td>0.325781</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022655</td>\n",
       "      <td>0.452369</td>\n",
       "      <td>0.260433</td>\n",
       "      <td>0.224170</td>\n",
       "      <td>0.060157</td>\n",
       "      <td>0.285095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours_since_installed_ma</th>\n",
       "      <td>0.058506</td>\n",
       "      <td>0.073770</td>\n",
       "      <td>0.266369</td>\n",
       "      <td>-0.025045</td>\n",
       "      <td>0.229785</td>\n",
       "      <td>0.022655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035472</td>\n",
       "      <td>-0.120826</td>\n",
       "      <td>-0.504087</td>\n",
       "      <td>-0.038457</td>\n",
       "      <td>0.047079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chests_reward_preceding_30_days</th>\n",
       "      <td>0.810140</td>\n",
       "      <td>0.300493</td>\n",
       "      <td>0.538212</td>\n",
       "      <td>0.447752</td>\n",
       "      <td>0.351481</td>\n",
       "      <td>0.452369</td>\n",
       "      <td>0.035472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136834</td>\n",
       "      <td>0.026103</td>\n",
       "      <td>0.068902</td>\n",
       "      <td>0.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_spins_preceding_7_days</th>\n",
       "      <td>0.187956</td>\n",
       "      <td>0.368017</td>\n",
       "      <td>0.126432</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>0.129724</td>\n",
       "      <td>0.260433</td>\n",
       "      <td>-0.120826</td>\n",
       "      <td>0.136834</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500875</td>\n",
       "      <td>0.208040</td>\n",
       "      <td>0.411977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_villages_completed_preceding_7_days</th>\n",
       "      <td>0.061099</td>\n",
       "      <td>0.238756</td>\n",
       "      <td>-0.121117</td>\n",
       "      <td>0.236906</td>\n",
       "      <td>-0.064288</td>\n",
       "      <td>0.224170</td>\n",
       "      <td>-0.504087</td>\n",
       "      <td>0.026103</td>\n",
       "      <td>0.500875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109440</td>\n",
       "      <td>0.183388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_card_xp</th>\n",
       "      <td>0.083262</td>\n",
       "      <td>0.092398</td>\n",
       "      <td>0.062466</td>\n",
       "      <td>0.084992</td>\n",
       "      <td>0.057428</td>\n",
       "      <td>0.060157</td>\n",
       "      <td>-0.038457</td>\n",
       "      <td>0.068902</td>\n",
       "      <td>0.208040</td>\n",
       "      <td>0.109440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.088315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_past_seen_price_oos_preceding_7_days</th>\n",
       "      <td>0.296680</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.289122</td>\n",
       "      <td>0.218036</td>\n",
       "      <td>0.222487</td>\n",
       "      <td>0.285095</td>\n",
       "      <td>0.047079</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.411977</td>\n",
       "      <td>0.183388</td>\n",
       "      <td>0.088315</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:45:06.171516Z",
     "start_time": "2024-12-26T21:45:06.146512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_selected = X_train[selected_features]\n",
    "X_dev_selected = X_dev[selected_features]"
   ],
   "id": "5222f5a174550f12",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Reasons why I used Gradient Boosting for feature selection:\n",
    "\n",
    "Random Forest is less suitable for feature selection because it builds trees independently, which means that if we have multiple correlated features, they can be selected in different trees and inflate the importance. GB instead works sequentially, this means that if we have 2 features that are highly correlated, GB selects one to avoid redundancy. The feature selected is the one that reduces the residual error at each step. Then once one feature in a correlated group is selected, the others offer diminishing returns in terms of residual reduction."
   ],
   "id": "41784d731d1258f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T21:45:06.179874Z",
     "start_time": "2024-12-26T21:45:06.176718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare assessment metrics\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    return {\"RMSE\": rmse}"
   ],
   "id": "ad610bbbf79a8eca",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here I'll use RF as baseline, and XGBoost, lightGBM and Catboost as learners. I do not use GB as learner to avoid bias.",
   "id": "98677734e40b4891"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-26T21:45:06.241072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Algorithms to evaluate (adaboost does not handle well multicolinearity)\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42), # baseline\n",
    "    \"XGBoost\": XGBRegressor(random_state=42),\n",
    "    \"LightGBM\": LGBMRegressor(verbose=0, random_state=42),\n",
    "    \"CatBoost\": CatBoostRegressor(verbose=0, random_state=42),\n",
    "}\n",
    "\n",
    "# Evaluate each model on dev set\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_dev_selected)\n",
    "    metrics = evaluate_metrics(y_dev, y_pred)\n",
    "    results[name] = metrics\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "    print()"
   ],
   "id": "118e139d9417a8dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate each model on train set to check overfitting\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    y_pred = model.predict(X_train_selected)\n",
    "    metrics = evaluate_metrics(y_train, y_pred)\n",
    "    results[name] = metrics\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "    print()"
   ],
   "id": "fbe65315f3d70e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So far it seems like lightGBM is the one that performs better, because of the lowest discrepancy between train and dev RMSE.",
   "id": "15a04c8d4762c603"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hyperparameter tuning on dev set through RandomSearchCV (for speed, as GridSearchCV takes time) - done on lightGBM.",
   "id": "d0f8e602456ecf4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lgbm_params = {\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"num_leaves\": [20, 31, 50],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"min_data_in_leaf\": [10, 20, 30]\n",
    "}"
   ],
   "id": "76eeceb61e7b715e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- learning_rate: Smaller values reduce overfitting but require more trees.\n",
    "- n_estimators: Number of boosting rounds.\n",
    "- max_depth: Maximum depth of trees.\n",
    "- num_leaves: Number of leaves in a tree. Lower values reduce overfitting.\n",
    "- min_data_in_leaf: Minimum samples in a leaf."
   ],
   "id": "728fa9a89a908578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "light_gbm = LGBMRegressor(verbose=0, random_state=42)\n",
    "light_gbm_random_search = RandomizedSearchCV(\n",
    "    estimator=light_gbm,\n",
    "    param_distributions=lgbm_params,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=0,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "light_gbm_random_search.fit(X_dev_selected, y_dev)\n",
    "print(\"Best Parameters for LightGBM:\", light_gbm_random_search.best_params_)\n",
    "print(\"Best CV RMSE:\", -light_gbm_random_search.best_score_)"
   ],
   "id": "13cb2561d1b4f03d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Predict on the test set:",
   "id": "c6350ea9f9e4a33f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_test = df_test.drop(columns=['org_price_usd_following_30_days'])\n",
    "y_test = df_test['org_price_usd_following_30_days']\n",
    "\n",
    "# Preprocess test data (using same logic)\n",
    "X_test_processed = preprocess_binning_and_encoding(\n",
    "    df=X_test,\n",
    "    target_column=\"village_id\",\n",
    "    categories_to_keep=village_ids_to_keep,\n",
    "    fit=False\n",
    ")\n",
    "\n",
    "X_test_processed = preprocess_binning_and_encoding(\n",
    "    df=X_test_processed,\n",
    "    target_column=\"weekday\",\n",
    "    categories_to_keep=weekdays_to_keep,\n",
    "    fit=False\n",
    ")\n",
    "\n",
    "X_test_log_transformed = log_transformer.transform(X_test_processed)\n",
    "X_test_cleaned = outlier_capper.transform(X_test_log_transformed, numerical_features)\n",
    "X_test_selected = X_test_cleaned[final_selected_features]\n",
    "X_test_selected = generate_interaction_terms(df=X_test_selected)\n",
    "X_test_selected_scaled = X_test_selected.copy()\n",
    "X_test_selected_scaled[numerical_to_scale] = scaler.transform(X_test_selected_scaled[numerical_to_scale])\n",
    "X_test_selected = X_test_selected_scaled[selected_features]"
   ],
   "id": "742122b076d4e798",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = LGBMRegressor(num_leaves=31, n_estimators=200, min_data_in_leaf=10, max_depth=5, learning_rate=0.1, random_state=42, verbose=0)\n",
    "model.fit(X_train_selected, y_train)\n",
    "y_pred = model.predict(X_test_selected)\n",
    "metrics = evaluate_metrics(y_test, y_pred)\n",
    "print(metrics)"
   ],
   "id": "fbef3bafcf356719",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since the distribution of y is very skewed, the RMSE needs to be assessed by portion of data.",
   "id": "18ff1deab5a39267"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Percentile-based scaling\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "p90 = np.percentile(y_test, 90)\n",
    "p10 = np.percentile(y_test, 10)\n",
    "relative_rmse_percentile = rmse / (p90 - p10)\n",
    "print(f\"Relative RMSE (Scaled by 90th-10th Percentile): {relative_rmse_percentile:.4f}\")"
   ],
   "id": "f9ccd4e9856d61bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The metric above confirms that the model performs well on the overall span of the data. The percentile based scaling says that the RMSE is < 1% between the 10th and the 90th percentile, meaning that it performs well on the majority of the target values, excluding outliers.",
   "id": "81e55a9b81b400f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- What are the three most important features that contributed to the prediction?",
   "id": "cfe4592da018904b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# top 3 most important features\n",
    "importances = model.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "selected_features = X_test_selected.columns[sorted_indices[:3]]\n",
    "selected_features"
   ],
   "id": "b5a0f891d5996047",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Further steps: if I had more time I would have refactored the code into a .py script, with another script to call with auxiliary functions. The model can be stored in a pickle file locally or in an S3 bucket together with encoders/scalers, for production purposes.",
   "id": "888f4939d01cff54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Recommendation task**\n",
    "\n",
    "We are interested in increasing the income from users. For that, we ran a randomized experiment where the population was given either a 10 usd offer or 2usd offer (see the \"treatment\" column), aiming to learn what offer should be given to a user. The experiment yielded a target variable named “org_price_usd_following_30_days_after_impact”, reflecting the result of the experiment in terms of income.\n",
    "1. For each user in the test data, set the treatment (either 10 or 2) that you believe would maximize the target variable (add a new column for that)\n",
    "2. What are the three most important features that contributed to the decision to give users a specific treatment?"
   ],
   "id": "a2d6dd1c556b191a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train = pd.read_csv('train_home_assignment_.csv', index_col=0)\n",
    "df_test = pd.read_csv('test_home_assignment.csv')"
   ],
   "id": "6cc563f427c7d94b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_train_recom = y_train.to_frame().join(df_train['org_price_usd_following_30_days_after_impact'], how='left')",
   "id": "1fea3058dd9a388b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_train_recom[['org_price_usd_following_30_days','org_price_usd_following_30_days_after_impact']].corr()",
   "id": "2923fed586802501",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dropping old label\n",
    "df_train.drop(columns=['org_price_usd_following_30_days'], inplace=True)\n",
    "df_test.drop(columns=['org_price_usd_following_30_days'], inplace=True)"
   ],
   "id": "50136f657c7ac9ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From this task I understand that I need to recommend a treatment to each new user of the test set under the constraint of maximizing org_price_usd_following_30_days_after_impact.",
   "id": "1ad2c3a2d26a20b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(df_train.groupby(['treatment'])['org_price_usd_following_30_days_after_impact'].sum())",
   "id": "d6cba5d0fa7ec81f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "By looking at the sum of org_price_usd_following_30_days_after_impact for each treatment, it seems like treatment does not have any disciminatory power, meaning including it as a further feature in a predictive model won't help much. Therefore, I'll approach this problem in the following way:\n",
    "- I'll train 2 models, one for each treatment, where the label is org_price_usd_following_30_days_after_impact.\n",
    "- I'll output 2 predictions for the test set, one per model\n",
    "- For each new user in the test set, I'll assign 10 if the prediction for 10 is higher, otherwise 2.\n",
    "- I'll compute the total predicted outcome based on this method and I'll compare it to a random baseline."
   ],
   "id": "24d703eacbe9e9d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I'll use the previous preprocessing and tuned learner as we have a correlation of 0.99 between previous and current label.",
   "id": "1708d957840d876"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_to_join = df_train[['treatment', 'org_price_usd_following_30_days_after_impact']]\n",
    "\n",
    "# Left join on the indices of X_train_selected\n",
    "X_train_selected_recom = X_train_selected.join(columns_to_join, how='left')"
   ],
   "id": "b07d8c8493def708",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into 10 and 2 treatment.\n",
    "df_train_10 = X_train_selected_recom[X_train_selected_recom.treatment == 10].drop(columns=['treatment'])\n",
    "df_train_2 = X_train_selected_recom[X_train_selected_recom.treatment == 2].drop(columns=['treatment'])\n",
    "\n",
    "# Separate data by treatment\n",
    "X_treatment_10 = df_train_10.drop(columns=['org_price_usd_following_30_days_after_impact'])\n",
    "y_treatment_10 = df_train_10['org_price_usd_following_30_days_after_impact']\n",
    "\n",
    "X_treatment_2 = df_train_2.drop(columns=['org_price_usd_following_30_days_after_impact'])\n",
    "y_treatment_2 = df_train_2['org_price_usd_following_30_days_after_impact']\n",
    "\n",
    "# Train models for each treatment\n",
    "model_10 = GradientBoostingRegressor()\n",
    "model_10.fit(X_treatment_10, y_treatment_10)\n",
    "\n",
    "model_2 = GradientBoostingRegressor()\n",
    "model_2.fit(X_treatment_2, y_treatment_2)\n",
    "\n"
   ],
   "id": "6af4b9a370b162fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#check if we have the same columns in train and test\n",
    "# Get the column sets\n",
    "X_test_selected_recom = X_test_selected.copy()\n",
    "train_columns = set(X_treatment_10.columns)\n",
    "test_columns = set(X_test_selected_recom.columns)\n",
    "\n",
    "# Check for equality\n",
    "if train_columns == test_columns:\n",
    "    print(\"The columns in df_train and df_test are the same.\")\n",
    "else:\n",
    "    print(\"The columns in df_train and df_test are different.\")\n",
    "\n",
    "    # Find columns in df_train but not in df_test\n",
    "    missing_in_test = train_columns - test_columns\n",
    "    if missing_in_test:\n",
    "        print(f\"Columns in df_train but not in df_test: {missing_in_test}\")\n",
    "\n",
    "    # Find columns in df_test but not in df_train\n",
    "    missing_in_train = test_columns - train_columns\n",
    "    if missing_in_train:\n",
    "        print(f\"Columns in df_test but not in df_train: {missing_in_train}\")"
   ],
   "id": "413cb181a5c70fdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predict for the test set\n",
    "y_pred_10 = model_10.predict(X_test_selected_recom)\n",
    "y_pred_2 = model_2.predict(X_test_selected_recom)\n",
    "\n",
    "# Assign treatment based on predicted outcomes\n",
    "X_test_selected_recom[\"recommended_treatment\"] = [10 if pred_10 > pred_2 else 2 for pred_10, pred_2 in zip(y_pred_10, y_pred_2)]"
   ],
   "id": "d2bc59d50ebecc37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate total predicted outcome for recommended treatments\n",
    "X_test_selected_recom[\"predicted_outcome\"] = [\n",
    "    pred_10 if treatment == 10 else pred_2\n",
    "    for treatment, pred_10, pred_2 in zip(X_test_selected_recom[\"recommended_treatment\"], y_pred_10, y_pred_2)\n",
    "]\n",
    "total_predicted_outcome = X_test_selected_recom[\"predicted_outcome\"].sum()\n",
    "print(f\"Total Predicted Outcome: {total_predicted_outcome}\")\n",
    "\n",
    "# Calculate uplift (compared to random assignment baseline)\n",
    "baseline_outcome = max(y_pred_10.mean(), y_pred_2.mean()) * len(X_test_selected_recom)\n",
    "uplift = total_predicted_outcome - baseline_outcome\n",
    "print(f\"Uplift: {uplift}\")"
   ],
   "id": "6d61ae1d47f6764",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(X_test_selected_recom.groupby([\"recommended_treatment\"])['predicted_outcome'].sum())",
   "id": "7060f4a5c669e954",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What are the three most important features that contributed to the decision to give users a specific treatment?",
   "id": "9bf31a19c33e9737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature importance for \\$10 treatment model\n",
    "importances_10 = model_10.feature_importances_\n",
    "features_10 = pd.DataFrame({\n",
    "    \"feature\": X_treatment_10.columns,\n",
    "    \"importance\": importances_10\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "print(\"Top features for \\$10 treatment:\")\n",
    "print(features_10.head(3))\n",
    "\n",
    "# Feature importance for \\$2 treatment model\n",
    "importances_2 = model_2.feature_importances_\n",
    "features_2 = pd.DataFrame({\n",
    "    \"feature\": X_treatment_2.columns,\n",
    "    \"importance\": importances_2\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "print(\"Top features for \\$2 treatment:\")\n",
    "print(features_2.head(3))\n"
   ],
   "id": "4e9c049839ffe693",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
